{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f7424da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-20 11:39:29.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mexample_file: /home/crosstyan/Code/hurricane_stuff/CMABSTdata/CH1950BST.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "\n",
    "class EndStatus(Enum):\n",
    "    DISSIPATED = 0\n",
    "    MOVE_OUT_OF_RESPONSIBILITY = 1\n",
    "    MERGED = 2\n",
    "    NEARLY_STATIONARY = 3\n",
    "\n",
    "\n",
    "class CycloneCategory(Enum):\n",
    "    BELOW_TD_OR_UNKNOWN = 0\n",
    "    TROPICAL_DEPRESSION = 1  # 热带低压 (TD, 10.8-17.1m/s)\n",
    "    TROPICAL_STORM = 2  # 热带风暴 (TS, 17.2-24.4 m/s)\n",
    "    SEVERE_TROPICAL_STORM = 3  # 强热带风暴 (STS, 24.5-32.6 m/s)\n",
    "    TYPHOON = 4  # 台风 (TY, 32.7-41.4 m/s)\n",
    "    SEVERE_TYPHOON = 5  # 强台风 (STY, 41.5-50.9 m/s)\n",
    "    SUPER_TYPHOON = 6  # 超强台风 (SuperTY, ≥51.0 m/s)\n",
    "    EXTRATROPICAL = 9  # 变性 (The change is complete)\n",
    "\n",
    "\n",
    "class HurricaneHeader(BaseModel):\n",
    "    data_type: int\n",
    "    country_code: int\n",
    "    data_count: int\n",
    "    hurricane_code: int\n",
    "    china_hurricane_code: int\n",
    "    end_status: EndStatus\n",
    "    time_interval_hr: int\n",
    "    hurricane_name: str\n",
    "    dataset_record_time: datetime\n",
    "\n",
    "\n",
    "class HurricaneEntry(BaseModel):\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "class Hurricane(BaseModel):\n",
    "    header: HurricaneHeader\n",
    "    entries: List[HurricaneEntry]\n",
    "\n",
    "\n",
    "script_folder = Path(os.getcwd())\n",
    "dataset_folder = script_folder / \"CMABSTdata\"\n",
    "\n",
    "# https://tcdata.typhoon.org.cn/zjljsjj.html\n",
    "# example_file = dataset_folder / \"CH2022BST.txt\"\n",
    "example_file = dataset_folder / \"CH1950BST.txt\"\n",
    "logger.info(f\"example_file: {example_file}\")\n",
    "\n",
    "\n",
    "def parse_header(line: str) -> HurricaneHeader:\n",
    "    entry = line.split()\n",
    "    data_type = int(entry[0])\n",
    "    country_code = int(entry[1])\n",
    "    data_count = int(entry[2])\n",
    "    hurricane_code = int(entry[3])\n",
    "    try:\n",
    "        china_hurricane_code = int(entry[4])\n",
    "    except ValueError:\n",
    "        # might be a tuple (a,b)\n",
    "        codes = entry[4].split(\",\")\n",
    "        china_hurricane_code = int(codes[0])\n",
    "    hurricane_end_enum = int(entry[5])\n",
    "    end_status = EndStatus(hurricane_end_enum)\n",
    "    time_interval_hr = int(entry[6])\n",
    "    hurricane_name = entry[7]\n",
    "    dataset_record_time = entry[8]\n",
    "    time_format = \"%Y%m%d\"\n",
    "    dataset_record_time = datetime.strptime(dataset_record_time, time_format)\n",
    "    return HurricaneHeader(data_type=data_type,\n",
    "                           country_code=country_code,\n",
    "                           data_count=data_count,\n",
    "                           hurricane_code=hurricane_code,\n",
    "                           china_hurricane_code=china_hurricane_code,\n",
    "                           end_status=end_status,\n",
    "                           time_interval_hr=time_interval_hr,\n",
    "                           hurricane_name=hurricane_name,\n",
    "                           dataset_record_time=dataset_record_time)\n",
    "\n",
    "\n",
    "def parse_entry(line: str) -> HurricaneEntry:\n",
    "    entry = line.split()\n",
    "    date_str = entry[0]\n",
    "    time_format = \"%Y%m%d%H\"\n",
    "    date = datetime.strptime(date_str, time_format)\n",
    "    category = int(entry[1])\n",
    "    hurricane_category = CycloneCategory(category)\n",
    "    latitude = float(int(entry[2])) / 10.0\n",
    "    longitude = float(int(entry[3])) / 10.0\n",
    "    # in hPa\n",
    "    lowest_pressure = int(entry[4])\n",
    "    # 2分钟平均近中心最大风速(MSW, m/s)\n",
    "    # WND=9 表示 MSW < 10m/s,\n",
    "    # WND=0 为缺测\n",
    "    wind_speed = int(entry[5])\n",
    "    # not sure about OWD\n",
    "    return HurricaneEntry(date=date,\n",
    "                          category=hurricane_category,\n",
    "                          latitude=latitude,\n",
    "                          longitude=longitude,\n",
    "                          lowest_pressure=lowest_pressure,\n",
    "                          wind_speed=wind_speed)\n",
    "\n",
    "\n",
    "def parse_dataset(filename: str | Path):\n",
    "    hurricanes: list[Hurricane] = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            while True:\n",
    "                # check if the line is empty\n",
    "                l = f.readline()\n",
    "                if not l:\n",
    "                    break\n",
    "                header = parse_header(l)\n",
    "                count = header.data_count\n",
    "                hurricane_entries = []\n",
    "                for i in range(count):\n",
    "                    entry = parse_entry(f.readline())\n",
    "                    hurricane_entries.append(entry)\n",
    "                hurricane = Hurricane(header=header, entries=hurricane_entries)\n",
    "                hurricanes.append(hurricane)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e} for {filename}\")\n",
    "        except IndexError as e:\n",
    "            logger.warning(f\"IndexError: {e} for {filename}\")\n",
    "        except EOFError:\n",
    "            logger.info(f\"EOFError for {filename}\")\n",
    "    return hurricanes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4b76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-20 11:39:30.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mtotal_dataset: 2469\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_dataset: list[Hurricane] = []\n",
    "\n",
    "for file in dataset_folder.glob(\"*.txt\"):\n",
    "    hurricanes = parse_dataset(file)\n",
    "    total_dataset.extend(hurricanes)\n",
    "\n",
    "logger.info(f\"total_dataset: {len(total_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a494943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatHurricaneEntry(BaseModel):\n",
    "    sample_id: int\n",
    "    name: str\n",
    "    china_hurricane_code: int\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "def flat_hurricane_entries(\n",
    "        hurricanes: list[Hurricane]) -> List[FlatHurricaneEntry]:\n",
    "    counter = 0\n",
    "    def flat_one(h: Hurricane, counter: int = counter):\n",
    "        name = h.header.hurricane_name\n",
    "        hurricane_code = h.header.hurricane_code\n",
    "        entries = h.entries\n",
    "        return [\n",
    "            FlatHurricaneEntry(sample_id=counter,\n",
    "                               name=name,\n",
    "                               china_hurricane_code=hurricane_code,\n",
    "                               date=e.date,\n",
    "                               category=e.category,\n",
    "                               latitude=e.latitude,\n",
    "                               longitude=e.longitude,\n",
    "                               lowest_pressure=e.lowest_pressure,\n",
    "                               wind_speed=e.wind_speed) for e in entries\n",
    "        ]\n",
    "\n",
    "    entries = []\n",
    "    for h in hurricanes:\n",
    "        entries.extend(flat_one(h, counter))\n",
    "        counter += 1\n",
    "    return entries\n",
    "\n",
    "\n",
    "flatten_entries = [\n",
    "    e.model_dump() for e in flat_hurricane_entries(total_dataset)\n",
    "]\n",
    "\n",
    "\n",
    "def entry_enum_to_number(entry: dict[str, any]) -> dict[str, any]:\n",
    "    entry['category'] = entry['category'].value\n",
    "    return entry\n",
    "\n",
    "\n",
    "flatten_entries_without_enum = [\n",
    "    entry_enum_to_number(e) for e in flatten_entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6794859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(flatten_entries_without_enum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c5c345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>name</th><th>china_hurricane_code</th><th>date</th><th>category</th><th>latitude</th><th>longitude</th><th>lowest_pressure</th><th>wind_speed</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>71705.0</td><td>&quot;71705&quot;</td><td>71705.0</td><td>&quot;71705&quot;</td><td>71705.0</td><td>71705.0</td><td>71705.0</td><td>71705.0</td><td>71705.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1238.613332</td><td>null</td><td>17.650066</td><td>&quot;1983-09-15 19:…</td><td>2.821588</td><td>20.902251</td><td>134.228423</td><td>986.340004</td><td>23.758218</td></tr><tr><td>&quot;std&quot;</td><td>712.786331</td><td>null</td><td>10.47291</td><td>null</td><td>2.330128</td><td>9.283707</td><td>16.616481</td><td>20.931241</td><td>15.36339</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>&quot;(nameless)&quot;</td><td>1.0</td><td>&quot;1949-01-13 00:…</td><td>0.0</td><td>0.5</td><td>95.0</td><td>870.0</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>621.0</td><td>null</td><td>9.0</td><td>&quot;1965-09-21 06:…</td><td>1.0</td><td>14.2</td><td>121.8</td><td>980.0</td><td>15.0</td></tr><tr><td>&quot;50%&quot;</td><td>1234.0</td><td>null</td><td>17.0</td><td>&quot;1981-08-21 00:…</td><td>2.0</td><td>19.3</td><td>132.5</td><td>995.0</td><td>20.0</td></tr><tr><td>&quot;75%&quot;</td><td>1869.0</td><td>null</td><td>25.0</td><td>&quot;2001-09-03 06:…</td><td>4.0</td><td>25.8</td><td>145.0</td><td>1001.0</td><td>33.0</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>&quot;Zola&quot;</td><td>53.0</td><td>&quot;2022-12-13 06:…</td><td>9.0</td><td>70.1</td><td>243.9</td><td>1022.0</td><td>110.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ sample_id ┆ name      ┆ china_hur ┆ … ┆ latitude  ┆ longitude ┆ lowest_pr ┆ wind_spe │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ricane_co ┆   ┆ ---       ┆ ---       ┆ essure    ┆ ed       │\n",
       "│ str       ┆ f64       ┆ str       ┆ de        ┆   ┆ f64       ┆ f64       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆ ---       ┆   ┆           ┆           ┆ f64       ┆ f64      │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 71705.0   ┆ 71705     ┆ 71705.0   ┆ … ┆ 71705.0   ┆ 71705.0   ┆ 71705.0   ┆ 71705.0  │\n",
       "│ null_coun ┆ 0.0       ┆ 0         ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ 1238.6133 ┆ null      ┆ 17.650066 ┆ … ┆ 20.902251 ┆ 134.22842 ┆ 986.34000 ┆ 23.75821 │\n",
       "│           ┆ 32        ┆           ┆           ┆   ┆           ┆ 3         ┆ 4         ┆ 8        │\n",
       "│ std       ┆ 712.78633 ┆ null      ┆ 10.47291  ┆ … ┆ 9.283707  ┆ 16.616481 ┆ 20.931241 ┆ 15.36339 │\n",
       "│           ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ min       ┆ 0.0       ┆ (nameless ┆ 1.0       ┆ … ┆ 0.5       ┆ 95.0      ┆ 870.0     ┆ 0.0      │\n",
       "│           ┆           ┆ )         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 25%       ┆ 621.0     ┆ null      ┆ 9.0       ┆ … ┆ 14.2      ┆ 121.8     ┆ 980.0     ┆ 15.0     │\n",
       "│ 50%       ┆ 1234.0    ┆ null      ┆ 17.0      ┆ … ┆ 19.3      ┆ 132.5     ┆ 995.0     ┆ 20.0     │\n",
       "│ 75%       ┆ 1869.0    ┆ null      ┆ 25.0      ┆ … ┆ 25.8      ┆ 145.0     ┆ 1001.0    ┆ 33.0     │\n",
       "│ max       ┆ 2468.0    ┆ Zola      ┆ 53.0      ┆ … ┆ 70.1      ┆ 243.9     ┆ 1022.0    ┆ 110.0    │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef8e4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>name</th><th>china_hurricane_code</th><th>date</th><th>category</th><th>latitude</th><th>longitude</th><th>lowest_pressure</th><th>wind_speed</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1307.737796</td><td>null</td><td>17.502265</td><td>&quot;1985-09-05 05:…</td><td>2.866664</td><td>20.73495</td><td>133.459558</td><td>984.894963</td><td>25.891893</td></tr><tr><td>&quot;std&quot;</td><td>699.270657</td><td>null</td><td>10.381192</td><td>null</td><td>2.121499</td><td>8.752285</td><td>16.292624</td><td>21.13435</td><td>14.21218</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>&quot;(nameless)&quot;</td><td>1.0</td><td>&quot;1949-01-15 00:…</td><td>0.0</td><td>0.5</td><td>95.0</td><td>870.0</td><td>8.0</td></tr><tr><td>&quot;25%&quot;</td><td>731.0</td><td>null</td><td>9.0</td><td>&quot;1968-06-01 00:…</td><td>1.0</td><td>14.5</td><td>121.2</td><td>975.0</td><td>15.0</td></tr><tr><td>&quot;50%&quot;</td><td>1337.0</td><td>null</td><td>17.0</td><td>&quot;1984-10-31 00:…</td><td>2.0</td><td>19.4</td><td>131.7</td><td>992.0</td><td>20.0</td></tr><tr><td>&quot;75%&quot;</td><td>1918.0</td><td>null</td><td>25.0</td><td>&quot;2003-06-17 18:…</td><td>4.0</td><td>25.5</td><td>143.9</td><td>1000.0</td><td>35.0</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>&quot;Zola&quot;</td><td>53.0</td><td>&quot;2022-12-13 06:…</td><td>9.0</td><td>70.1</td><td>243.9</td><td>1016.0</td><td>110.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic ┆ sample_id ┆ name      ┆ china_hur ┆ … ┆ latitude ┆ longitude ┆ lowest_pr ┆ wind_spee │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ricane_co ┆   ┆ ---      ┆ ---       ┆ essure    ┆ d         │\n",
       "│ str       ┆ f64       ┆ str       ┆ de        ┆   ┆ f64      ┆ f64       ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆           ┆ ---       ┆   ┆          ┆           ┆ f64       ┆ f64       │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆          ┆           ┆           ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count     ┆ 65796.0   ┆ 65796     ┆ 65796.0   ┆ … ┆ 65796.0  ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   │\n",
       "│ null_coun ┆ 0.0       ┆ 0         ┆ 0.0       ┆ … ┆ 0.0      ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ mean      ┆ 1307.7377 ┆ null      ┆ 17.502265 ┆ … ┆ 20.73495 ┆ 133.45955 ┆ 984.89496 ┆ 25.891893 │\n",
       "│           ┆ 96        ┆           ┆           ┆   ┆          ┆ 8         ┆ 3         ┆           │\n",
       "│ std       ┆ 699.27065 ┆ null      ┆ 10.381192 ┆ … ┆ 8.752285 ┆ 16.292624 ┆ 21.13435  ┆ 14.21218  │\n",
       "│           ┆ 7         ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ min       ┆ 0.0       ┆ (nameless ┆ 1.0       ┆ … ┆ 0.5      ┆ 95.0      ┆ 870.0     ┆ 8.0       │\n",
       "│           ┆           ┆ )         ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 25%       ┆ 731.0     ┆ null      ┆ 9.0       ┆ … ┆ 14.5     ┆ 121.2     ┆ 975.0     ┆ 15.0      │\n",
       "│ 50%       ┆ 1337.0    ┆ null      ┆ 17.0      ┆ … ┆ 19.4     ┆ 131.7     ┆ 992.0     ┆ 20.0      │\n",
       "│ 75%       ┆ 1918.0    ┆ null      ┆ 25.0      ┆ … ┆ 25.5     ┆ 143.9     ┆ 1000.0    ┆ 35.0      │\n",
       "│ max       ┆ 2468.0    ┆ Zola      ┆ 53.0      ┆ … ┆ 70.1     ┆ 243.9     ┆ 1016.0    ┆ 110.0     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df.filter(df[\"wind_speed\"] != 0)\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778a4d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(df_filtered[\"wind_speed\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd5a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-20 11:39:34.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mdevice: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e0d9c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-20 11:39:34.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m1949-01-14 12:00:00 -> (1.2246467991473532e-16, -1.0) (0.2386727660059501, 0.9711000518829505)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "time = df[\"date\"][6]\n",
    "assert isinstance(time, datetime)\n",
    "# use sin/cos to normalize the day in a year and the hour in a day\n",
    "\n",
    "def sinusoidal_hour_in_day(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the hour of day from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the hour from the datetime object\n",
    "    hour = dt.hour\n",
    "\n",
    "    # Calculate the radians for the given hour\n",
    "    radians_per_hour = 2 * math.pi / 24\n",
    "    hour_in_radians = hour * radians_per_hour\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(hour_in_radians), math.cos(hour_in_radians)\n",
    "\n",
    "def sinusoidal_day_in_year(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the day of year from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the day of year from the datetime object\n",
    "    day_of_year = dt.timetuple().tm_yday\n",
    "\n",
    "    # Handle leap years\n",
    "    year_length = 366 if dt.year % 4 == 0 and (dt.year % 100 != 0 or dt.year % 400 == 0) else 365\n",
    "\n",
    "    # Calculate the radians for the given day of year\n",
    "    radians_per_day = 2 * math.pi / year_length\n",
    "    day_in_radians = day_of_year * radians_per_day\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(day_in_radians), math.cos(day_in_radians)\n",
    "\n",
    "logger.info(f\"{time} -> {sinusoidal_hour_in_day(time)} {sinusoidal_day_in_year(time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8084585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# longitude and latitude\n",
    "lat_long_scaler = MinMaxScaler()\n",
    "latitude = lat_long_scaler.fit_transform(df_filtered[\"latitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "longitude = lat_long_scaler.fit_transform(df_filtered[\"longitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# wind speed\n",
    "wind_scaler = StandardScaler()\n",
    "wind_speed = wind_scaler.fit_transform(df_filtered[\"wind_speed\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# lowest pressure\n",
    "lowest_pressure_scaler = StandardScaler()\n",
    "lowest_pressure = lowest_pressure_scaler.fit_transform(df_filtered[\"lowest_pressure\"].to_numpy().reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c9f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_normalized_time = df_filtered.with_columns([\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[0]).alias(\"sin_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[1]).alias(\"cos_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[0]).alias(\"sin_hour_in_day\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[1]).alias(\"cos_hour_in_day\"),\n",
    "    pl.Series(\"latitude_norm\", latitude),\n",
    "    pl.Series(\"longitude_norm\", longitude),\n",
    "    pl.Series(\"wind_speed_norm\", wind_speed),\n",
    "    pl.Series(\"lowest_pressure_norm\", lowest_pressure),\n",
    "])\n",
    "\n",
    "df_features = with_normalized_time.select([\n",
    "    \"sample_id\",\n",
    "    \"sin_day_in_year\",\n",
    "    \"cos_day_in_year\",\n",
    "    \"sin_hour_in_day\",\n",
    "    \"cos_hour_in_day\",\n",
    "    \"latitude_norm\",\n",
    "    \"longitude_norm\",\n",
    "    \"wind_speed_norm\",\n",
    "    \"lowest_pressure_norm\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65de21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>sin_day_in_year</th><th>cos_day_in_year</th><th>sin_hour_in_day</th><th>cos_hour_in_day</th><th>latitude_norm</th><th>longitude_norm</th><th>wind_speed_norm</th><th>lowest_pressure_norm</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1307.737796</td><td>-0.498177</td><td>-0.264362</td><td>0.006157</td><td>0.003637</td><td>0.290732</td><td>0.258291</td><td>7.5743e-17</td><td>-2.4023e-16</td></tr><tr><td>&quot;std&quot;</td><td>699.270657</td><td>0.550623</td><td>0.615432</td><td>0.705854</td><td>0.708332</td><td>0.125751</td><td>0.10942</td><td>1.000008</td><td>1.000008</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>-0.999991</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>0.0</td><td>0.0</td><td>-1.258922</td><td>-5.43645</td></tr><tr><td>&quot;25%&quot;</td><td>731.0</td><td>-0.927542</td><td>-0.809017</td><td>0.0</td><td>-0.707107</td><td>0.201149</td><td>0.175957</td><td>-0.766383</td><td>-0.468197</td></tr><tr><td>&quot;50%&quot;</td><td>1337.0</td><td>-0.699458</td><td>-0.413279</td><td>1.2246e-16</td><td>6.1232e-17</td><td>0.271552</td><td>0.246474</td><td>-0.41457</td><td>0.336187</td></tr><tr><td>&quot;75%&quot;</td><td>1918.0</td><td>-0.263665</td><td>0.209315</td><td>1.0</td><td>1.0</td><td>0.359195</td><td>0.328408</td><td>0.640871</td><td>0.71472</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>0.999991</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>5.918075</td><td>1.471788</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ sample_id ┆ sin_day_i ┆ cos_day_i ┆ … ┆ latitude_ ┆ longitude ┆ wind_spee ┆ lowest_p │\n",
       "│ ---       ┆ ---       ┆ n_year    ┆ n_year    ┆   ┆ norm      ┆ _norm     ┆ d_norm    ┆ ressure_ │\n",
       "│ str       ┆ f64       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ norm     │\n",
       "│           ┆           ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   ┆ … ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   ┆ 65796.0  │\n",
       "│ null_coun ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ 1307.7377 ┆ -0.498177 ┆ -0.264362 ┆ … ┆ 0.290732  ┆ 0.258291  ┆ 7.5743e-1 ┆ -2.4023e │\n",
       "│           ┆ 96        ┆           ┆           ┆   ┆           ┆           ┆ 7         ┆ -16      │\n",
       "│ std       ┆ 699.27065 ┆ 0.550623  ┆ 0.615432  ┆ … ┆ 0.125751  ┆ 0.10942   ┆ 1.000008  ┆ 1.000008 │\n",
       "│           ┆ 7         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ min       ┆ 0.0       ┆ -0.999991 ┆ -1.0      ┆ … ┆ 0.0       ┆ 0.0       ┆ -1.258922 ┆ -5.43645 │\n",
       "│ 25%       ┆ 731.0     ┆ -0.927542 ┆ -0.809017 ┆ … ┆ 0.201149  ┆ 0.175957  ┆ -0.766383 ┆ -0.46819 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
       "│ 50%       ┆ 1337.0    ┆ -0.699458 ┆ -0.413279 ┆ … ┆ 0.271552  ┆ 0.246474  ┆ -0.41457  ┆ 0.336187 │\n",
       "│ 75%       ┆ 1918.0    ┆ -0.263665 ┆ 0.209315  ┆ … ┆ 0.359195  ┆ 0.328408  ┆ 0.640871  ┆ 0.71472  │\n",
       "│ max       ┆ 2468.0    ┆ 0.999991  ┆ 1.0       ┆ … ┆ 1.0       ┆ 1.0       ┆ 5.918075  ┆ 1.471788 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "103e1de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65796"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c00c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_840939/1895117370.py:44: DeprecationWarning: `group_by` iteration will change to always return group identifiers as tuples. Pass `by` as a list to silence this warning, e.g. `group_by(['sample_id'])`.\n",
      "  filtered = filter(filter_out_short_sequence, grouped)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1599, 20, 8), (1599, 20, 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1599, 10, 8), (1599, 10, 4))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "from functools import reduce\n",
    "# group by sample_id and iterate over the groups\n",
    "grouped = df_features.group_by(\"sample_id\")\n",
    "from typing import Iterable, Iterator, Tuple, Union\n",
    "\n",
    "EXPECTED_TIMESTAMP_COUNT = 20\n",
    "\n",
    "\n",
    "def filter_out_short_sequence(id_and_df: tuple[int, pl.DataFrame]) -> bool:\n",
    "    return id_and_df[1].height >= EXPECTED_TIMESTAMP_COUNT\n",
    "\n",
    "\n",
    "def pad_or_truncate(\n",
    "        id_and_df: tuple[int, pl.DataFrame]) -> tuple[pl.Series, pl.DataFrame]:\n",
    "    group_id, df = id_and_df\n",
    "    if df.height < EXPECTED_TIMESTAMP_COUNT:\n",
    "        # pad with zeros\n",
    "        diff = EXPECTED_TIMESTAMP_COUNT - df.height\n",
    "        mask = pl.Series(\"mask\", [True] * df.height + [False] * diff)\n",
    "        zeros = pl.DataFrame({\n",
    "            \"sample_id\": [group_id] * diff,\n",
    "            \"sin_day_in_year\": [0.0] * diff,\n",
    "            \"cos_day_in_year\": [0.0] * diff,\n",
    "            \"sin_hour_in_day\": [0.0] * diff,\n",
    "            \"cos_hour_in_day\": [0.0] * diff,\n",
    "            \"latitude_norm\": [0.0] * diff,\n",
    "            \"longitude_norm\": [0.0] * diff,\n",
    "            \"wind_speed_norm\": [0.0] * diff,\n",
    "            \"lowest_pressure_norm\": [0.0] * diff,\n",
    "        })\n",
    "        stacked = df.vstack(zeros)\n",
    "        # sort by date\n",
    "        return mask, stacked.sort(\"date\")\n",
    "    elif df.height >= EXPECTED_TIMESTAMP_COUNT:\n",
    "        # truncate\n",
    "        mask = pl.Series(\"mask\", [True] * EXPECTED_TIMESTAMP_COUNT)\n",
    "        return mask, df.head(EXPECTED_TIMESTAMP_COUNT)\n",
    "    else:\n",
    "        mask = pl.Series(\"mask\", [True] * df.height)\n",
    "        return mask, df\n",
    "\n",
    "\n",
    "filtered = filter(filter_out_short_sequence, grouped)\n",
    "padded = map(pad_or_truncate, filtered)\n",
    "\n",
    "\n",
    "# for some reason, the reduce function is not working\n",
    "def to_tensor(\n",
    "        id_and_df: Iterable[tuple[int,\n",
    "                                  pl.DataFrame]]) -> tuple[NDArray, NDArray]:\n",
    "    init_mask, init_data = np.empty(\n",
    "        (0, EXPECTED_TIMESTAMP_COUNT, 1)), np.empty(\n",
    "            (0, EXPECTED_TIMESTAMP_COUNT, df_features.width))\n",
    "    for mask, df in id_and_df:\n",
    "        current_data = df.to_numpy()\n",
    "        current_mask = np.expand_dims(mask.to_numpy(), axis=-1)\n",
    "        try:\n",
    "            new_data = np.vstack(\n",
    "                (init_data, np.expand_dims(current_data, axis=0)))\n",
    "            new_mask = np.vstack(\n",
    "                (init_mask, np.expand_dims(current_mask, axis=0)))\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e}\")\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data.shape}, current_data: {current_data.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask.shape}, current_mask: {current_mask.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data}, current_data: {current_data}\")\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask}, current_mask: {current_mask}\")\n",
    "\n",
    "        init_data, init_mask = new_data, new_mask\n",
    "    return init_data, init_mask\n",
    "\n",
    "\n",
    "data_with_id, mask = to_tensor(padded)\n",
    "# remove the sample_id column\n",
    "features = data_with_id[:, :, 1:]\n",
    "display((features.shape, mask.shape))\n",
    "\n",
    "# y_train should be the last 4 features\n",
    "y_train = features[:, -10:, -4:]\n",
    "X_train = features[:, :10, :]\n",
    "display((X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfc35b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65796, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>category</th><th>len</th></tr><tr><td>i64</td><td>u32</td></tr></thead><tbody><tr><td>5</td><td>4945</td></tr><tr><td>0</td><td>4133</td></tr><tr><td>3</td><td>11435</td></tr><tr><td>2</td><td>11125</td></tr><tr><td>9</td><td>3165</td></tr><tr><td>1</td><td>17650</td></tr><tr><td>6</td><td>3914</td></tr><tr><td>4</td><td>9429</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 2)\n",
       "┌──────────┬───────┐\n",
       "│ category ┆ len   │\n",
       "│ ---      ┆ ---   │\n",
       "│ i64      ┆ u32   │\n",
       "╞══════════╪═══════╡\n",
       "│ 5        ┆ 4945  │\n",
       "│ 0        ┆ 4133  │\n",
       "│ 3        ┆ 11435 │\n",
       "│ 2        ┆ 11125 │\n",
       "│ 9        ┆ 3165  │\n",
       "│ 1        ┆ 17650 │\n",
       "│ 6        ┆ 3914  │\n",
       "│ 4        ┆ 9429  │\n",
       "└──────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# one hot encoding for category\n",
    "# with sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "category = df_filtered[\"category\"].to_numpy()\n",
    "category = category.reshape(-1, 1)\n",
    "encoder = OneHotEncoder()\n",
    "category = encoder.fit_transform(category)\n",
    "display(category.shape)\n",
    "# get the count of each category\n",
    "category_count = df_filtered.group_by(\"category\").len()\n",
    "display(category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e00c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>count</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>2469.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>26.648846</td></tr><tr><td>&quot;std&quot;</td><td>14.383362</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td></tr><tr><td>&quot;25%&quot;</td><td>15.0</td></tr><tr><td>&quot;50%&quot;</td><td>25.0</td></tr><tr><td>&quot;75%&quot;</td><td>36.0</td></tr><tr><td>&quot;max&quot;</td><td>97.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 2)\n",
       "┌────────────┬───────────┐\n",
       "│ statistic  ┆ count     │\n",
       "│ ---        ┆ ---       │\n",
       "│ str        ┆ f64       │\n",
       "╞════════════╪═══════════╡\n",
       "│ count      ┆ 2469.0    │\n",
       "│ null_count ┆ 0.0       │\n",
       "│ mean       ┆ 26.648846 │\n",
       "│ std        ┆ 14.383362 │\n",
       "│ min        ┆ 1.0       │\n",
       "│ 25%        ┆ 15.0      │\n",
       "│ 50%        ┆ 25.0      │\n",
       "│ 75%        ┆ 36.0      │\n",
       "│ max        ┆ 97.0      │\n",
       "└────────────┴───────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the average count of samples per hurricane (by sample_id)\n",
    "average_samples_per_hurricane = df_features.group_by(\"sample_id\").agg(pl.col(\"sample_id\").count().alias(\"count\")).select(\"count\")\n",
    "average_samples_per_hurricane.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a6287e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(average_samples_per_hurricane[\"count\"], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20284c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 11:39:36.254912: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 11:39:37.224033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[32m2024-03-20 11:39:37.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1m[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\u001b[0m\n",
      "2024-03-20 11:39:37.747124: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-20 11:39:37.748458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-20 11:39:37.748687: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "logger.info(tf.config.list_physical_devices(\"GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77b804da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "display(K.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db1af2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crosstyan/.conda/envs/cross/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m18,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m4,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)          │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,276</span> (149.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,276\u001b[0m (149.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,276</span> (149.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,276\u001b[0m (149.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Masking, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import AdamW, Lion\n",
    "\n",
    "dim_timesteps = X_train.shape[1]\n",
    "dim_features = X_train.shape[2]\n",
    "dim_output_features = y_train.shape[2]\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(dim_timesteps, dim_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(GRU(16, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(dim_output_features, activation=\"linear\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Lion(learning_rate=0.0015), loss=\"mean_squared_error\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d359152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 2s/step - loss: 0.6545\n",
      "Epoch 1: val_loss improved from inf to 0.54193, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.6597 - val_loss: 0.5419\n",
      "Epoch 2/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5265\n",
      "Epoch 2: val_loss improved from 0.54193 to 0.49669, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5530 - val_loss: 0.4967\n",
      "Epoch 3/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.6730\n",
      "Epoch 3: val_loss improved from 0.49669 to 0.46927, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5774 - val_loss: 0.4693\n",
      "Epoch 4/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5873\n",
      "Epoch 4: val_loss improved from 0.46927 to 0.44338, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.5155 - val_loss: 0.4434\n",
      "Epoch 5/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4725\n",
      "Epoch 5: val_loss improved from 0.44338 to 0.42909, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4626 - val_loss: 0.4291\n",
      "Epoch 6/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4804\n",
      "Epoch 6: val_loss improved from 0.42909 to 0.41027, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4610 - val_loss: 0.4103\n",
      "Epoch 7/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4443\n",
      "Epoch 7: val_loss improved from 0.41027 to 0.39333, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4352 - val_loss: 0.3933\n",
      "Epoch 8/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4349\n",
      "Epoch 8: val_loss improved from 0.39333 to 0.39040, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.4179 - val_loss: 0.3904\n",
      "Epoch 9/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.4542\n",
      "Epoch 9: val_loss improved from 0.39040 to 0.36947, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4112 - val_loss: 0.3695\n",
      "Epoch 10/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3836\n",
      "Epoch 10: val_loss improved from 0.36947 to 0.36622, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3921 - val_loss: 0.3662\n",
      "Epoch 11/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3809\n",
      "Epoch 11: val_loss improved from 0.36622 to 0.34178, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.3863 - val_loss: 0.3418\n",
      "Epoch 12/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3950\n",
      "Epoch 12: val_loss did not improve from 0.34178\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3754 - val_loss: 0.3554\n",
      "Epoch 13/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3734\n",
      "Epoch 13: val_loss improved from 0.34178 to 0.32271, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3573 - val_loss: 0.3227\n",
      "Epoch 14/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3389\n",
      "Epoch 14: val_loss did not improve from 0.32271\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3429 - val_loss: 0.3391\n",
      "Epoch 15/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3316\n",
      "Epoch 15: val_loss improved from 0.32271 to 0.30931, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3337 - val_loss: 0.3093\n",
      "Epoch 16/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3183\n",
      "Epoch 16: val_loss did not improve from 0.30931\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3320 - val_loss: 0.3137\n",
      "Epoch 17/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3201\n",
      "Epoch 17: val_loss improved from 0.30931 to 0.29254, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3195 - val_loss: 0.2925\n",
      "Epoch 18/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2951\n",
      "Epoch 18: val_loss did not improve from 0.29254\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3094 - val_loss: 0.3065\n",
      "Epoch 19/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3433\n",
      "Epoch 19: val_loss improved from 0.29254 to 0.27838, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3229 - val_loss: 0.2784\n",
      "Epoch 20/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3150\n",
      "Epoch 20: val_loss improved from 0.27838 to 0.27032, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2939 - val_loss: 0.2703\n",
      "Epoch 21/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.2935\n",
      "Epoch 21: val_loss improved from 0.27032 to 0.26021, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2925 - val_loss: 0.2602\n",
      "Epoch 22/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2685\n",
      "Epoch 22: val_loss did not improve from 0.26021\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2831 - val_loss: 0.2646\n",
      "Epoch 23/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2602\n",
      "Epoch 23: val_loss improved from 0.26021 to 0.24805, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2744 - val_loss: 0.2480\n",
      "Epoch 24/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.3006\n",
      "Epoch 24: val_loss improved from 0.24805 to 0.24479, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2778 - val_loss: 0.2448\n",
      "Epoch 25/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2716\n",
      "Epoch 25: val_loss improved from 0.24479 to 0.23192, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2654 - val_loss: 0.2319\n",
      "Epoch 26/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2655\n",
      "Epoch 26: val_loss did not improve from 0.23192\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2627 - val_loss: 0.2422\n",
      "Epoch 27/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2535\n",
      "Epoch 27: val_loss improved from 0.23192 to 0.22570, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2544 - val_loss: 0.2257\n",
      "Epoch 28/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2353\n",
      "Epoch 28: val_loss did not improve from 0.22570\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2505 - val_loss: 0.2345\n",
      "Epoch 29/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2653\n",
      "Epoch 29: val_loss improved from 0.22570 to 0.22365, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2528 - val_loss: 0.2237\n",
      "Epoch 30/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2378\n",
      "Epoch 30: val_loss did not improve from 0.22365\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2529 - val_loss: 0.2289\n",
      "Epoch 31/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2479\n",
      "Epoch 31: val_loss improved from 0.22365 to 0.22177, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2399 - val_loss: 0.2218\n",
      "Epoch 32/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2221\n",
      "Epoch 32: val_loss did not improve from 0.22177\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2306 - val_loss: 0.2230\n",
      "Epoch 33/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2372\n",
      "Epoch 33: val_loss improved from 0.22177 to 0.21870, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2441 - val_loss: 0.2187\n",
      "Epoch 34/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2405\n",
      "Epoch 34: val_loss did not improve from 0.21870\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2392 - val_loss: 0.2216\n",
      "Epoch 35/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2035\n",
      "Epoch 35: val_loss did not improve from 0.21870\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2279 - val_loss: 0.2206\n",
      "Epoch 36/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2374\n",
      "Epoch 36: val_loss did not improve from 0.21870\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2317 - val_loss: 0.2210\n",
      "Epoch 37/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2177\n",
      "Epoch 37: val_loss did not improve from 0.21870\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2259 - val_loss: 0.2189\n",
      "Epoch 38/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2123\n",
      "Epoch 38: val_loss improved from 0.21870 to 0.21273, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2213 - val_loss: 0.2127\n",
      "Epoch 39/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1999\n",
      "Epoch 39: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2171 - val_loss: 0.2140\n",
      "Epoch 40/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2320\n",
      "Epoch 40: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2244 - val_loss: 0.2150\n",
      "Epoch 41/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.2102\n",
      "Epoch 41: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2153 - val_loss: 0.2130\n",
      "Epoch 42/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2225\n",
      "Epoch 42: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2226 - val_loss: 0.2174\n",
      "Epoch 43/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2317\n",
      "Epoch 43: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2185 - val_loss: 0.2202\n",
      "Epoch 44/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2114\n",
      "Epoch 44: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2160 - val_loss: 0.2141\n",
      "Epoch 45/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1951\n",
      "Epoch 45: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2103 - val_loss: 0.2160\n",
      "Epoch 46/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2333\n",
      "Epoch 46: val_loss did not improve from 0.21273\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2224 - val_loss: 0.2138\n",
      "Epoch 47/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2093\n",
      "Epoch 47: val_loss improved from 0.21273 to 0.21038, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2143 - val_loss: 0.2104\n",
      "Epoch 48/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2029\n",
      "Epoch 48: val_loss did not improve from 0.21038\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2092 - val_loss: 0.2131\n",
      "Epoch 49/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1956\n",
      "Epoch 49: val_loss did not improve from 0.21038\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2005 - val_loss: 0.2130\n",
      "Epoch 50/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1933\n",
      "Epoch 50: val_loss improved from 0.21038 to 0.20718, saving model to hurricane_lstm.keras\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.2038 - val_loss: 0.2072\n",
      "Epoch 51/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1974\n",
      "Epoch 51: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1982 - val_loss: 0.2133\n",
      "Epoch 52/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2192\n",
      "Epoch 52: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2087 - val_loss: 0.2119\n",
      "Epoch 53/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1803\n",
      "Epoch 53: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1919 - val_loss: 0.2179\n",
      "Epoch 54/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1868\n",
      "Epoch 54: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1993 - val_loss: 0.2169\n",
      "Epoch 55/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2189\n",
      "Epoch 55: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1977 - val_loss: 0.2099\n",
      "Epoch 56/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1781\n",
      "Epoch 56: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1915 - val_loss: 0.2139\n",
      "Epoch 57/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1966\n",
      "Epoch 57: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1959 - val_loss: 0.2096\n",
      "Epoch 58/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1961\n",
      "Epoch 58: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1957 - val_loss: 0.2155\n",
      "Epoch 59/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.2042\n",
      "Epoch 59: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1962 - val_loss: 0.2160\n",
      "Epoch 60/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1832\n",
      "Epoch 60: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1910 - val_loss: 0.2183\n",
      "Epoch 61/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1770\n",
      "Epoch 61: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1944 - val_loss: 0.2144\n",
      "Epoch 62/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1831\n",
      "Epoch 62: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1837 - val_loss: 0.2153\n",
      "Epoch 63/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1753\n",
      "Epoch 63: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1836 - val_loss: 0.2165\n",
      "Epoch 64/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1859\n",
      "Epoch 64: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1902 - val_loss: 0.2178\n",
      "Epoch 65/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1887\n",
      "Epoch 65: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1816 - val_loss: 0.2200\n",
      "Epoch 66/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1675\n",
      "Epoch 66: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1802 - val_loss: 0.2171\n",
      "Epoch 67/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1918\n",
      "Epoch 67: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1858 - val_loss: 0.2158\n",
      "Epoch 68/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1925\n",
      "Epoch 68: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1835 - val_loss: 0.2228\n",
      "Epoch 69/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1869\n",
      "Epoch 69: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1828 - val_loss: 0.2202\n",
      "Epoch 70/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1790\n",
      "Epoch 70: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1790 - val_loss: 0.2160\n",
      "Epoch 71/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1551\n",
      "Epoch 71: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1731 - val_loss: 0.2191\n",
      "Epoch 72/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1830\n",
      "Epoch 72: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1740 - val_loss: 0.2212\n",
      "Epoch 73/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2056\n",
      "Epoch 73: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1860 - val_loss: 0.2212\n",
      "Epoch 74/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1776\n",
      "Epoch 74: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1787 - val_loss: 0.2196\n",
      "Epoch 75/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1745\n",
      "Epoch 75: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1771 - val_loss: 0.2225\n",
      "Epoch 76/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.1868\n",
      "Epoch 76: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1756 - val_loss: 0.2290\n",
      "Epoch 77/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1847\n",
      "Epoch 77: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1715 - val_loss: 0.2211\n",
      "Epoch 78/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1625\n",
      "Epoch 78: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1682 - val_loss: 0.2255\n",
      "Epoch 79/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1784\n",
      "Epoch 79: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1745 - val_loss: 0.2292\n",
      "Epoch 80/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1589\n",
      "Epoch 80: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1657 - val_loss: 0.2280\n",
      "Epoch 81/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1774\n",
      "Epoch 81: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1696 - val_loss: 0.2328\n",
      "Epoch 82/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1618\n",
      "Epoch 82: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1668 - val_loss: 0.2321\n",
      "Epoch 83/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1413\n",
      "Epoch 83: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1624 - val_loss: 0.2355\n",
      "Epoch 84/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1546\n",
      "Epoch 84: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1627 - val_loss: 0.2281\n",
      "Epoch 85/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1451\n",
      "Epoch 85: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1593 - val_loss: 0.2407\n",
      "Epoch 86/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1722\n",
      "Epoch 86: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1630 - val_loss: 0.2295\n",
      "Epoch 87/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1642\n",
      "Epoch 87: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1592 - val_loss: 0.2315\n",
      "Epoch 88/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1463\n",
      "Epoch 88: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1541 - val_loss: 0.2394\n",
      "Epoch 89/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1870\n",
      "Epoch 89: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1645 - val_loss: 0.2393\n",
      "Epoch 90/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1689\n",
      "Epoch 90: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1591 - val_loss: 0.2364\n",
      "Epoch 91/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1656\n",
      "Epoch 91: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1577 - val_loss: 0.2327\n",
      "Epoch 92/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1586\n",
      "Epoch 92: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1580 - val_loss: 0.2385\n",
      "Epoch 93/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1556\n",
      "Epoch 93: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1548 - val_loss: 0.2387\n",
      "Epoch 94/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1458\n",
      "Epoch 94: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1502 - val_loss: 0.2354\n",
      "Epoch 95/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1514\n",
      "Epoch 95: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1515 - val_loss: 0.2323\n",
      "Epoch 96/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1562\n",
      "Epoch 96: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1490 - val_loss: 0.2321\n",
      "Epoch 97/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1794\n",
      "Epoch 97: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1576 - val_loss: 0.2431\n",
      "Epoch 98/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1506\n",
      "Epoch 98: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1514 - val_loss: 0.2364\n",
      "Epoch 99/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1422\n",
      "Epoch 99: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1471 - val_loss: 0.2397\n",
      "Epoch 100/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1492\n",
      "Epoch 100: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1481 - val_loss: 0.2429\n",
      "Epoch 101/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1501\n",
      "Epoch 101: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1496 - val_loss: 0.2373\n",
      "Epoch 102/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1622\n",
      "Epoch 102: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1507 - val_loss: 0.2403\n",
      "Epoch 103/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1432\n",
      "Epoch 103: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1450 - val_loss: 0.2421\n",
      "Epoch 104/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1240\n",
      "Epoch 104: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1407 - val_loss: 0.2362\n",
      "Epoch 105/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1654\n",
      "Epoch 105: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1520 - val_loss: 0.2408\n",
      "Epoch 106/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1498\n",
      "Epoch 106: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1465 - val_loss: 0.2421\n",
      "Epoch 107/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1275\n",
      "Epoch 107: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1387 - val_loss: 0.2438\n",
      "Epoch 108/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1442\n",
      "Epoch 108: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1451 - val_loss: 0.2355\n",
      "Epoch 109/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1172\n",
      "Epoch 109: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1326 - val_loss: 0.2398\n",
      "Epoch 110/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1358\n",
      "Epoch 110: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1414 - val_loss: 0.2400\n",
      "Epoch 111/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1454\n",
      "Epoch 111: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1404 - val_loss: 0.2421\n",
      "Epoch 112/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1289\n",
      "Epoch 112: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1345 - val_loss: 0.2483\n",
      "Epoch 113/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1411\n",
      "Epoch 113: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1409 - val_loss: 0.2497\n",
      "Epoch 114/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1388\n",
      "Epoch 114: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1351 - val_loss: 0.2432\n",
      "Epoch 115/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1372\n",
      "Epoch 115: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1379 - val_loss: 0.2517\n",
      "Epoch 116/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1409\n",
      "Epoch 116: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1351 - val_loss: 0.2498\n",
      "Epoch 117/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1234\n",
      "Epoch 117: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1307 - val_loss: 0.2447\n",
      "Epoch 118/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1462\n",
      "Epoch 118: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1362 - val_loss: 0.2533\n",
      "Epoch 119/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1307\n",
      "Epoch 119: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1325 - val_loss: 0.2493\n",
      "Epoch 120/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1323\n",
      "Epoch 120: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1309 - val_loss: 0.2429\n",
      "Epoch 121/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1358\n",
      "Epoch 121: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1305 - val_loss: 0.2635\n",
      "Epoch 122/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1238\n",
      "Epoch 122: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1280 - val_loss: 0.2466\n",
      "Epoch 123/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1220\n",
      "Epoch 123: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1263 - val_loss: 0.2426\n",
      "Epoch 124/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1284\n",
      "Epoch 124: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1270 - val_loss: 0.2525\n",
      "Epoch 125/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1101\n",
      "Epoch 125: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1236 - val_loss: 0.2495\n",
      "Epoch 126/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1218\n",
      "Epoch 126: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1247 - val_loss: 0.2543\n",
      "Epoch 127/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1326\n",
      "Epoch 127: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1247 - val_loss: 0.2534\n",
      "Epoch 128/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1097\n",
      "Epoch 128: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1199 - val_loss: 0.2643\n",
      "Epoch 129/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1381\n",
      "Epoch 129: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1267 - val_loss: 0.2646\n",
      "Epoch 130/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1194\n",
      "Epoch 130: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1175 - val_loss: 0.2589\n",
      "Epoch 131/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1158\n",
      "Epoch 131: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1194 - val_loss: 0.2665\n",
      "Epoch 132/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1233\n",
      "Epoch 132: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1220 - val_loss: 0.2656\n",
      "Epoch 133/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1363\n",
      "Epoch 133: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1251 - val_loss: 0.2562\n",
      "Epoch 134/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1326\n",
      "Epoch 134: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1255 - val_loss: 0.2553\n",
      "Epoch 135/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1201\n",
      "Epoch 135: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1177 - val_loss: 0.2603\n",
      "Epoch 136/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1182\n",
      "Epoch 136: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1173 - val_loss: 0.2629\n",
      "Epoch 137/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1115\n",
      "Epoch 137: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1170 - val_loss: 0.2603\n",
      "Epoch 138/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1202\n",
      "Epoch 138: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1187 - val_loss: 0.2638\n",
      "Epoch 139/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1120\n",
      "Epoch 139: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1113 - val_loss: 0.2485\n",
      "Epoch 140/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1249\n",
      "Epoch 140: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1170 - val_loss: 0.2617\n",
      "Epoch 141/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1163\n",
      "Epoch 141: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1126 - val_loss: 0.2613\n",
      "Epoch 142/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1039\n",
      "Epoch 142: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1098 - val_loss: 0.2586\n",
      "Epoch 143/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1138\n",
      "Epoch 143: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1137 - val_loss: 0.2673\n",
      "Epoch 144/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1066\n",
      "Epoch 144: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1086 - val_loss: 0.2655\n",
      "Epoch 145/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1074\n",
      "Epoch 145: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1072 - val_loss: 0.2640\n",
      "Epoch 146/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1043\n",
      "Epoch 146: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1066 - val_loss: 0.2576\n",
      "Epoch 147/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1019\n",
      "Epoch 147: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1060 - val_loss: 0.2647\n",
      "Epoch 148/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1091\n",
      "Epoch 148: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1079 - val_loss: 0.2587\n",
      "Epoch 149/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0928\n",
      "Epoch 149: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1009 - val_loss: 0.2681\n",
      "Epoch 150/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0981\n",
      "Epoch 150: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1042 - val_loss: 0.2633\n",
      "Epoch 151/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1025\n",
      "Epoch 151: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1047 - val_loss: 0.2664\n",
      "Epoch 152/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0940\n",
      "Epoch 152: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1040 - val_loss: 0.2638\n",
      "Epoch 153/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0967\n",
      "Epoch 153: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0998 - val_loss: 0.2606\n",
      "Epoch 154/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0906\n",
      "Epoch 154: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0986 - val_loss: 0.2660\n",
      "Epoch 155/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0942\n",
      "Epoch 155: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0990 - val_loss: 0.2631\n",
      "Epoch 156/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0995\n",
      "Epoch 156: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1001 - val_loss: 0.2591\n",
      "Epoch 157/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0989\n",
      "Epoch 157: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0991 - val_loss: 0.2668\n",
      "Epoch 158/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0939\n",
      "Epoch 158: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0992 - val_loss: 0.2663\n",
      "Epoch 159/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1020\n",
      "Epoch 159: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1000 - val_loss: 0.2570\n",
      "Epoch 160/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1000\n",
      "Epoch 160: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0995 - val_loss: 0.2588\n",
      "Epoch 161/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0789\n",
      "Epoch 161: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0912 - val_loss: 0.2602\n",
      "Epoch 162/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0963\n",
      "Epoch 162: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0965 - val_loss: 0.2531\n",
      "Epoch 163/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0853\n",
      "Epoch 163: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0911 - val_loss: 0.2606\n",
      "Epoch 164/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0984\n",
      "Epoch 164: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0970 - val_loss: 0.2584\n",
      "Epoch 165/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0936\n",
      "Epoch 165: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0960 - val_loss: 0.2693\n",
      "Epoch 166/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0935\n",
      "Epoch 166: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0942 - val_loss: 0.2730\n",
      "Epoch 167/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0899\n",
      "Epoch 167: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0939 - val_loss: 0.2793\n",
      "Epoch 168/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0891\n",
      "Epoch 168: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0917 - val_loss: 0.2743\n",
      "Epoch 169/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0833\n",
      "Epoch 169: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0913 - val_loss: 0.2718\n",
      "Epoch 170/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0818\n",
      "Epoch 170: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0874 - val_loss: 0.2752\n",
      "Epoch 171/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0919\n",
      "Epoch 171: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0899 - val_loss: 0.2727\n",
      "Epoch 172/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0955\n",
      "Epoch 172: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0918 - val_loss: 0.2763\n",
      "Epoch 173/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0868\n",
      "Epoch 173: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0863 - val_loss: 0.2694\n",
      "Epoch 174/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0853\n",
      "Epoch 174: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0874 - val_loss: 0.2780\n",
      "Epoch 175/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0884\n",
      "Epoch 175: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0870 - val_loss: 0.2830\n",
      "Epoch 176/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0881\n",
      "Epoch 176: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0871 - val_loss: 0.2829\n",
      "Epoch 177/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0961\n",
      "Epoch 177: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0906 - val_loss: 0.2744\n",
      "Epoch 178/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0906\n",
      "Epoch 178: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0869 - val_loss: 0.2760\n",
      "Epoch 179/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0859\n",
      "Epoch 179: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0867 - val_loss: 0.2762\n",
      "Epoch 180/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0826\n",
      "Epoch 180: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0841 - val_loss: 0.2703\n",
      "Epoch 181/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0777\n",
      "Epoch 181: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0821 - val_loss: 0.2717\n",
      "Epoch 182/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0830\n",
      "Epoch 182: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0843 - val_loss: 0.2774\n",
      "Epoch 183/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0865\n",
      "Epoch 183: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0838 - val_loss: 0.2747\n",
      "Epoch 184/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0743\n",
      "Epoch 184: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0798 - val_loss: 0.2799\n",
      "Epoch 185/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0795\n",
      "Epoch 185: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0815 - val_loss: 0.2684\n",
      "Epoch 186/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0819\n",
      "Epoch 186: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0808 - val_loss: 0.2709\n",
      "Epoch 187/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0909\n",
      "Epoch 187: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0844 - val_loss: 0.2817\n",
      "Epoch 188/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0769\n",
      "Epoch 188: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0796 - val_loss: 0.2784\n",
      "Epoch 189/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0777\n",
      "Epoch 189: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0809 - val_loss: 0.2837\n",
      "Epoch 190/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0782\n",
      "Epoch 190: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0792 - val_loss: 0.2830\n",
      "Epoch 191/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0769\n",
      "Epoch 191: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0787 - val_loss: 0.2782\n",
      "Epoch 192/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0719\n",
      "Epoch 192: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0764 - val_loss: 0.2824\n",
      "Epoch 193/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0748\n",
      "Epoch 193: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0769 - val_loss: 0.2951\n",
      "Epoch 194/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0767\n",
      "Epoch 194: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0769 - val_loss: 0.2768\n",
      "Epoch 195/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0762\n",
      "Epoch 195: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0776 - val_loss: 0.2779\n",
      "Epoch 196/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0761\n",
      "Epoch 196: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0783 - val_loss: 0.2760\n",
      "Epoch 197/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0715\n",
      "Epoch 197: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0744 - val_loss: 0.2699\n",
      "Epoch 198/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0781\n",
      "Epoch 198: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0785 - val_loss: 0.2796\n",
      "Epoch 199/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0729\n",
      "Epoch 199: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0746 - val_loss: 0.2678\n",
      "Epoch 200/200\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0774\n",
      "Epoch 200: val_loss did not improve from 0.20718\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0748 - val_loss: 0.2670\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"hurricane_lstm.keras\", save_best_only=True, monitor=\"val_loss\", mode=\"min\", verbose=1\n",
    ")\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=256, validation_split=0.2, callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7842e2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# use the model to predict the next 10 time steps\n",
    "# use the last 10 time steps from the training set\n",
    "X_test = X_train[-1, :, :]\n",
    "X_test = X_test.reshape(1, X_test.shape[0], X_test.shape[1])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "403aa27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[133.28067 , 138.04439 , 969.8505  ,  37.558186],\n",
       "       [134.43597 , 140.02562 , 968.56177 ,  38.06454 ],\n",
       "       [133.16565 , 138.69261 , 974.08514 ,  33.985302],\n",
       "       [134.01442 , 132.767   , 979.3082  ,  30.592905],\n",
       "       [136.88528 , 128.45422 , 983.3507  ,  27.989304],\n",
       "       [136.95135 , 128.44997 , 985.59796 ,  26.456995],\n",
       "       [136.14777 , 123.73232 , 986.22565 ,  25.819372],\n",
       "       [135.21445 , 121.55591 , 984.711   ,  27.175379],\n",
       "       [137.74936 , 119.73177 , 976.4964  ,  32.350407],\n",
       "       [141.50917 , 118.10136 , 971.5906  ,  34.105812]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape\n",
    "# reverse the normalization\n",
    "wind_pred = wind_scaler.inverse_transform(y_pred[:, :, 2])\n",
    "pressure_pred = lowest_pressure_scaler.inverse_transform(y_pred[:, :, 3])\n",
    "lat_pred = lat_long_scaler.inverse_transform(y_pred[:, :, 0])\n",
    "long_pred = lat_long_scaler.inverse_transform(y_pred[:, :, 1])\n",
    "# display((wind_pred, pressure_pred, lat_pred, long_pred))\n",
    "pred = np.vstack((lat_pred, long_pred, pressure_pred, wind_pred)).T\n",
    "display(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
