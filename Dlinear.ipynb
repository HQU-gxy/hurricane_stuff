{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-12 07:51:24.607\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mexample_file: d:\\毕业论文\\数据集\\hurricane_stuff-1\\CMABSTdata\\CH1950BST.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "\n",
    "class EndStatus(Enum):\n",
    "    DISSIPATED = 0\n",
    "    MOVE_OUT_OF_RESPONSIBILITY = 1\n",
    "    MERGED = 2\n",
    "    NEARLY_STATIONARY = 3\n",
    "\n",
    "\n",
    "class CycloneCategory(Enum):\n",
    "    BELOW_TD_OR_UNKNOWN = 0\n",
    "    TROPICAL_DEPRESSION = 1  # 热带低压 (TD, 10.8-17.1m/s)\n",
    "    TROPICAL_STORM = 2  # 热带风暴 (TS, 17.2-24.4 m/s)\n",
    "    SEVERE_TROPICAL_STORM = 3  # 强热带风暴 (STS, 24.5-32.6 m/s)\n",
    "    TYPHOON = 4  # 台风 (TY, 32.7-41.4 m/s)\n",
    "    SEVERE_TYPHOON = 5  # 强台风 (STY, 41.5-50.9 m/s)\n",
    "    SUPER_TYPHOON = 6  # 超强台风 (SuperTY, ≥51.0 m/s)\n",
    "    EXTRATROPICAL = 9  # 变性 (The change is complete)\n",
    "\n",
    "\n",
    "class HurricaneHeader(BaseModel):\n",
    "    data_type: int\n",
    "    country_code: int\n",
    "    data_count: int\n",
    "    hurricane_code: int\n",
    "    china_hurricane_code: int\n",
    "    end_status: EndStatus\n",
    "    time_interval_hr: int\n",
    "    hurricane_name: str\n",
    "    dataset_record_time: datetime\n",
    "\n",
    "\n",
    "class HurricaneEntry(BaseModel):\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "class Hurricane(BaseModel):\n",
    "    header: HurricaneHeader\n",
    "    entries: List[HurricaneEntry]\n",
    "\n",
    "\n",
    "script_folder = Path(os.getcwd())\n",
    "dataset_folder = script_folder / \"CMABSTdata\"\n",
    "\n",
    "# https://tcdata.typhoon.org.cn/zjljsjj.html\n",
    "# example_file = dataset_folder / \"CH2022BST.txt\"\n",
    "example_file = dataset_folder / \"CH1950BST.txt\"\n",
    "logger.info(f\"example_file: {example_file}\")\n",
    "\n",
    "\n",
    "def parse_header(line: str) -> HurricaneHeader:\n",
    "    entry = line.split()\n",
    "    data_type = int(entry[0])\n",
    "    country_code = int(entry[1])\n",
    "    data_count = int(entry[2])\n",
    "    hurricane_code = int(entry[3])\n",
    "    try:\n",
    "        china_hurricane_code = int(entry[4])\n",
    "    except ValueError:\n",
    "        # might be a tuple (a,b)\n",
    "        codes = entry[4].split(\",\")\n",
    "        china_hurricane_code = int(codes[0])\n",
    "    hurricane_end_enum = int(entry[5])\n",
    "    end_status = EndStatus(hurricane_end_enum)\n",
    "    time_interval_hr = int(entry[6])\n",
    "    hurricane_name = entry[7]\n",
    "    dataset_record_time = entry[8]\n",
    "    time_format = \"%Y%m%d\"\n",
    "    dataset_record_time = datetime.strptime(dataset_record_time, time_format)\n",
    "    return HurricaneHeader(data_type=data_type,\n",
    "                           country_code=country_code,\n",
    "                           data_count=data_count,\n",
    "                           hurricane_code=hurricane_code,\n",
    "                           china_hurricane_code=china_hurricane_code,\n",
    "                           end_status=end_status,\n",
    "                           time_interval_hr=time_interval_hr,\n",
    "                           hurricane_name=hurricane_name,\n",
    "                           dataset_record_time=dataset_record_time)\n",
    "\n",
    "\n",
    "def parse_entry(line: str) -> HurricaneEntry:\n",
    "    entry = line.split()\n",
    "    date_str = entry[0]\n",
    "    time_format = \"%Y%m%d%H\"\n",
    "    date = datetime.strptime(date_str, time_format)\n",
    "    category = int(entry[1])\n",
    "    hurricane_category = CycloneCategory(category)\n",
    "    latitude = float(int(entry[2])) / 10.0\n",
    "    longitude = float(int(entry[3])) / 10.0\n",
    "    # in hPa\n",
    "    lowest_pressure = int(entry[4])\n",
    "    # 2分钟平均近中心最大风速(MSW, m/s)\n",
    "    # WND=9 表示 MSW < 10m/s,\n",
    "    # WND=0 为缺测\n",
    "    wind_speed = int(entry[5])\n",
    "    # not sure about OWD\n",
    "    return HurricaneEntry(date=date,\n",
    "                          category=hurricane_category,\n",
    "                          latitude=latitude,\n",
    "                          longitude=longitude,\n",
    "                          lowest_pressure=lowest_pressure,\n",
    "                          wind_speed=wind_speed)\n",
    "\n",
    "\n",
    "def parse_dataset(filename):\n",
    "    hurricanes: list[Hurricane] = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            while True:\n",
    "                # check if the line is empty\n",
    "                l = f.readline()\n",
    "                if not l:\n",
    "                    break\n",
    "                header = parse_header(l)\n",
    "                count = header.data_count\n",
    "                hurricane_entries = []\n",
    "                for i in range(count):\n",
    "                    entry = parse_entry(f.readline())\n",
    "                    hurricane_entries.append(entry)\n",
    "                hurricane = Hurricane(header=header, entries=hurricane_entries)\n",
    "                hurricanes.append(hurricane)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e} for {filename}\")\n",
    "        except IndexError as e:\n",
    "            logger.warning(f\"IndexError: {e} for {filename}\")\n",
    "        except EOFError:\n",
    "            logger.info(f\"EOFError for {filename}\")\n",
    "    return hurricanes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-12 07:51:27.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mtotal_dataset: 2469\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_dataset: list[Hurricane] = []\n",
    "\n",
    "for file in dataset_folder.glob(\"*.txt\"):\n",
    "    hurricanes = parse_dataset(file)\n",
    "    total_dataset.extend(hurricanes)\n",
    "\n",
    "logger.info(f\"total_dataset: {len(total_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatHurricaneEntry(BaseModel):\n",
    "    sample_id: int\n",
    "    name: str\n",
    "    china_hurricane_code: int\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "def flat_hurricane_entries(\n",
    "        hurricanes: list[Hurricane]) -> List[FlatHurricaneEntry]:\n",
    "    counter = 0\n",
    "    def flat_one(h: Hurricane, counter: int = counter):\n",
    "        name = h.header.hurricane_name\n",
    "        hurricane_code = h.header.hurricane_code\n",
    "        entries = h.entries\n",
    "        return [\n",
    "            FlatHurricaneEntry(sample_id=counter,\n",
    "                               name=name,\n",
    "                               china_hurricane_code=hurricane_code,\n",
    "                               date=e.date,\n",
    "                               category=e.category,\n",
    "                               latitude=e.latitude,\n",
    "                               longitude=e.longitude,\n",
    "                               lowest_pressure=e.lowest_pressure,\n",
    "                               wind_speed=e.wind_speed) for e in entries\n",
    "        ]\n",
    "\n",
    "    entries = []\n",
    "    for h in hurricanes:\n",
    "        entries.extend(flat_one(h, counter))\n",
    "        counter += 1\n",
    "    return entries\n",
    "\n",
    "\n",
    "flatten_entries = [\n",
    "    e.model_dump() for e in flat_hurricane_entries(total_dataset)\n",
    "]\n",
    "\n",
    "\n",
    "def entry_enum_to_number(entry: dict[str, any]) -> dict[str, any]:\n",
    "    entry['category'] = entry['category'].value\n",
    "    return entry\n",
    "\n",
    "\n",
    "flatten_entries_without_enum = [\n",
    "    entry_enum_to_number(e) for e in flatten_entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>name</th><th>china_hurricane_code</th><th>date</th><th>category</th><th>latitude</th><th>longitude</th><th>lowest_pressure</th><th>wind_speed</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1307.737796</td><td>null</td><td>17.502265</td><td>&quot;1985-09-05 05:…</td><td>2.866664</td><td>20.73495</td><td>133.459558</td><td>984.894963</td><td>25.891893</td></tr><tr><td>&quot;std&quot;</td><td>699.270657</td><td>null</td><td>10.381192</td><td>null</td><td>2.121499</td><td>8.752285</td><td>16.292624</td><td>21.13435</td><td>14.21218</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>&quot;(nameless)&quot;</td><td>1.0</td><td>&quot;1949-01-15 00:…</td><td>0.0</td><td>0.5</td><td>95.0</td><td>870.0</td><td>8.0</td></tr><tr><td>&quot;25%&quot;</td><td>731.0</td><td>null</td><td>9.0</td><td>&quot;1968-06-01 00:…</td><td>1.0</td><td>14.5</td><td>121.2</td><td>975.0</td><td>15.0</td></tr><tr><td>&quot;50%&quot;</td><td>1337.0</td><td>null</td><td>17.0</td><td>&quot;1984-10-31 00:…</td><td>2.0</td><td>19.4</td><td>131.7</td><td>992.0</td><td>20.0</td></tr><tr><td>&quot;75%&quot;</td><td>1918.0</td><td>null</td><td>25.0</td><td>&quot;2003-06-17 18:…</td><td>4.0</td><td>25.5</td><td>143.9</td><td>1000.0</td><td>35.0</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>&quot;Zola&quot;</td><td>53.0</td><td>&quot;2022-12-13 06:…</td><td>9.0</td><td>70.1</td><td>243.9</td><td>1016.0</td><td>110.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic ┆ sample_id ┆ name      ┆ china_hur ┆ … ┆ latitude ┆ longitude ┆ lowest_pr ┆ wind_spee │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ricane_co ┆   ┆ ---      ┆ ---       ┆ essure    ┆ d         │\n",
       "│ str       ┆ f64       ┆ str       ┆ de        ┆   ┆ f64      ┆ f64       ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆           ┆ ---       ┆   ┆          ┆           ┆ f64       ┆ f64       │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆          ┆           ┆           ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count     ┆ 65796.0   ┆ 65796     ┆ 65796.0   ┆ … ┆ 65796.0  ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   │\n",
       "│ null_coun ┆ 0.0       ┆ 0         ┆ 0.0       ┆ … ┆ 0.0      ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ mean      ┆ 1307.7377 ┆ null      ┆ 17.502265 ┆ … ┆ 20.73495 ┆ 133.45955 ┆ 984.89496 ┆ 25.891893 │\n",
       "│           ┆ 96        ┆           ┆           ┆   ┆          ┆ 8         ┆ 3         ┆           │\n",
       "│ std       ┆ 699.27065 ┆ null      ┆ 10.381192 ┆ … ┆ 8.752285 ┆ 16.292624 ┆ 21.13435  ┆ 14.21218  │\n",
       "│           ┆ 7         ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ min       ┆ 0.0       ┆ (nameless ┆ 1.0       ┆ … ┆ 0.5      ┆ 95.0      ┆ 870.0     ┆ 8.0       │\n",
       "│           ┆           ┆ )         ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 25%       ┆ 731.0     ┆ null      ┆ 9.0       ┆ … ┆ 14.5     ┆ 121.2     ┆ 975.0     ┆ 15.0      │\n",
       "│ 50%       ┆ 1337.0    ┆ null      ┆ 17.0      ┆ … ┆ 19.4     ┆ 131.7     ┆ 992.0     ┆ 20.0      │\n",
       "│ 75%       ┆ 1918.0    ┆ null      ┆ 25.0      ┆ … ┆ 25.5     ┆ 143.9     ┆ 1000.0    ┆ 35.0      │\n",
       "│ max       ┆ 2468.0    ┆ Zola      ┆ 53.0      ┆ … ┆ 70.1     ┆ 243.9     ┆ 1016.0    ┆ 110.0     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame(flatten_entries_without_enum)\n",
    "df_filtered = df.filter(df[\"wind_speed\"] != 0)\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-12 07:51:29.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m1949-01-14 12:00:00 -> (1.2246467991473532e-16, -1.0) (0.2386727660059501, 0.9711000518829505)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "time = df[\"date\"][6]\n",
    "assert isinstance(time, datetime)\n",
    "# use sin/cos to normalize the day in a year and the hour in a day\n",
    "\n",
    "def sinusoidal_hour_in_day(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the hour of day from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the hour from the datetime object\n",
    "    hour = dt.hour\n",
    "\n",
    "    # Calculate the radians for the given hour\n",
    "    radians_per_hour = 2 * math.pi / 24\n",
    "    hour_in_radians = hour * radians_per_hour\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(hour_in_radians), math.cos(hour_in_radians)\n",
    "\n",
    "def sinusoidal_day_in_year(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the day of year from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the day of year from the datetime object\n",
    "    day_of_year = dt.timetuple().tm_yday\n",
    "\n",
    "    # Handle leap years\n",
    "    year_length = 366 if dt.year % 4 == 0 and (dt.year % 100 != 0 or dt.year % 400 == 0) else 365\n",
    "\n",
    "    # Calculate the radians for the given day of year\n",
    "    radians_per_day = 2 * math.pi / year_length\n",
    "    day_in_radians = day_of_year * radians_per_day\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(day_in_radians), math.cos(day_in_radians)\n",
    "\n",
    "logger.info(f\"{time} -> {sinusoidal_hour_in_day(time)} {sinusoidal_day_in_year(time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# longitude and latitude\n",
    "lat_long_scaler = MinMaxScaler()\n",
    "latitude = lat_long_scaler.fit_transform(df_filtered[\"latitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "longitude = lat_long_scaler.fit_transform(df_filtered[\"longitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# wind speed\n",
    "wind_scaler = StandardScaler()\n",
    "wind_speed = wind_scaler.fit_transform(df_filtered[\"wind_speed\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# lowest pressure\n",
    "lowest_pressure_scaler = StandardScaler()\n",
    "lowest_pressure = lowest_pressure_scaler.fit_transform(df_filtered[\"lowest_pressure\"].to_numpy().reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_normalized_time = df_filtered.with_columns([\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[0]).alias(\"sin_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[1]).alias(\"cos_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[0]).alias(\"sin_hour_in_day\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[1]).alias(\"cos_hour_in_day\"),\n",
    "    pl.Series(\"latitude_norm\", latitude),\n",
    "    pl.Series(\"longitude_norm\", longitude),\n",
    "    pl.Series(\"wind_speed_norm\", wind_speed),\n",
    "    pl.Series(\"lowest_pressure_norm\", lowest_pressure),\n",
    "])\n",
    "\n",
    "df_features = with_normalized_time.select([\n",
    "    \"sample_id\",\n",
    "    \"sin_day_in_year\",\n",
    "    \"cos_day_in_year\",\n",
    "    \"sin_hour_in_day\",\n",
    "    \"cos_hour_in_day\",\n",
    "    \"latitude_norm\",\n",
    "    \"longitude_norm\",\n",
    "    \"wind_speed_norm\",\n",
    "    \"lowest_pressure_norm\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_15480\\1895117370.py:44: DeprecationWarning: `group_by` iteration will change to always return group identifiers as tuples. Pass `by` as a list to silence this warning, e.g. `group_by(['sample_id'])`.\n",
      "  filtered = filter(filter_out_short_sequence, grouped)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1599, 20, 8), (1599, 20, 1))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1599, 10, 8), (1599, 10, 4))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "from functools import reduce\n",
    "# group by sample_id and iterate over the groups\n",
    "grouped = df_features.group_by(\"sample_id\")\n",
    "from typing import Iterable, Iterator, Tuple, Union\n",
    "\n",
    "EXPECTED_TIMESTAMP_COUNT = 20\n",
    "\n",
    "\n",
    "def filter_out_short_sequence(id_and_df: tuple[int, pl.DataFrame]) -> bool:\n",
    "    return id_and_df[1].height >= EXPECTED_TIMESTAMP_COUNT\n",
    "\n",
    "\n",
    "def pad_or_truncate(\n",
    "        id_and_df: tuple[int, pl.DataFrame]) -> tuple[pl.Series, pl.DataFrame]:\n",
    "    group_id, df = id_and_df\n",
    "    if df.height < EXPECTED_TIMESTAMP_COUNT:\n",
    "        # pad with zeros\n",
    "        diff = EXPECTED_TIMESTAMP_COUNT - df.height\n",
    "        mask = pl.Series(\"mask\", [True] * df.height + [False] * diff)\n",
    "        zeros = pl.DataFrame({\n",
    "            \"sample_id\": [group_id] * diff,\n",
    "            \"sin_day_in_year\": [0.0] * diff,\n",
    "            \"cos_day_in_year\": [0.0] * diff,\n",
    "            \"sin_hour_in_day\": [0.0] * diff,\n",
    "            \"cos_hour_in_day\": [0.0] * diff,\n",
    "            \"latitude_norm\": [0.0] * diff,\n",
    "            \"longitude_norm\": [0.0] * diff,\n",
    "            \"wind_speed_norm\": [0.0] * diff,\n",
    "            \"lowest_pressure_norm\": [0.0] * diff,\n",
    "        })\n",
    "        stacked = df.vstack(zeros)\n",
    "        # sort by date\n",
    "        return mask, stacked.sort(\"date\")\n",
    "    elif df.height >= EXPECTED_TIMESTAMP_COUNT:\n",
    "        # truncate\n",
    "        mask = pl.Series(\"mask\", [True] * EXPECTED_TIMESTAMP_COUNT)\n",
    "        return mask, df.head(EXPECTED_TIMESTAMP_COUNT)\n",
    "    else:\n",
    "        mask = pl.Series(\"mask\", [True] * df.height)\n",
    "        return mask, df\n",
    "\n",
    "\n",
    "filtered = filter(filter_out_short_sequence, grouped)\n",
    "padded = map(pad_or_truncate, filtered)\n",
    "\n",
    "\n",
    "# for some reason, the reduce function is not working\n",
    "def to_tensor(\n",
    "        id_and_df: Iterable[tuple[int,\n",
    "                                  pl.DataFrame]]) -> tuple[NDArray, NDArray]:\n",
    "    init_mask, init_data = np.empty(\n",
    "        (0, EXPECTED_TIMESTAMP_COUNT, 1)), np.empty(\n",
    "            (0, EXPECTED_TIMESTAMP_COUNT, df_features.width))\n",
    "    for mask, df in id_and_df:\n",
    "        current_data = df.to_numpy()\n",
    "        current_mask = np.expand_dims(mask.to_numpy(), axis=-1)\n",
    "        try:\n",
    "            new_data = np.vstack(\n",
    "                (init_data, np.expand_dims(current_data, axis=0)))\n",
    "            new_mask = np.vstack(\n",
    "                (init_mask, np.expand_dims(current_mask, axis=0)))\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e}\")\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data.shape}, current_data: {current_data.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask.shape}, current_mask: {current_mask.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data}, current_data: {current_data}\")\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask}, current_mask: {current_mask}\")\n",
    "\n",
    "        init_data, init_mask = new_data, new_mask\n",
    "    return init_data, init_mask\n",
    "\n",
    "\n",
    "data_with_id, mask = to_tensor(padded)\n",
    "# remove the sample_id column\n",
    "features = data_with_id[:, :, 1:]\n",
    "display((features.shape, mask.shape))\n",
    "\n",
    "# y_train should be the last 4 features\n",
    "y_train = features[:, -10:, -4:]\n",
    "X_train = features[:, :10, :]\n",
    "display((X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    DLinear\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = configs.individual\n",
    "        self.channels = configs.enc_in\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "            self.Linear_Decoder = nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "                self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "                self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "                self.Linear_Decoder.append(nn.Linear(self.seq_len,self.pred_len))\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Decoder = nn.Linear(self.seq_len,self.pred_len)\n",
    "            self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "            self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])\n",
    "                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0,2,1) # to [Batch, Output length, Channel]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
