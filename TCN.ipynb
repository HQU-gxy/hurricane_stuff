{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-23 02:44:08.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mexample_file: /home/crosstyan/code/hurricane_stuff/CMABSTdata/CH1950BST.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "\n",
    "class EndStatus(Enum):\n",
    "    DISSIPATED = 0\n",
    "    MOVE_OUT_OF_RESPONSIBILITY = 1\n",
    "    MERGED = 2\n",
    "    NEARLY_STATIONARY = 3\n",
    "\n",
    "\n",
    "class CycloneCategory(Enum):\n",
    "    BELOW_TD_OR_UNKNOWN = 0\n",
    "    TROPICAL_DEPRESSION = 1  # 热带低压 (TD, 10.8-17.1m/s)\n",
    "    TROPICAL_STORM = 2  # 热带风暴 (TS, 17.2-24.4 m/s)\n",
    "    SEVERE_TROPICAL_STORM = 3  # 强热带风暴 (STS, 24.5-32.6 m/s)\n",
    "    TYPHOON = 4  # 台风 (TY, 32.7-41.4 m/s)\n",
    "    SEVERE_TYPHOON = 5  # 强台风 (STY, 41.5-50.9 m/s)\n",
    "    SUPER_TYPHOON = 6  # 超强台风 (SuperTY, ≥51.0 m/s)\n",
    "    EXTRATROPICAL = 9  # 变性 (The change is complete)\n",
    "\n",
    "\n",
    "class HurricaneHeader(BaseModel):\n",
    "    data_type: int\n",
    "    country_code: int\n",
    "    data_count: int\n",
    "    hurricane_code: int\n",
    "    china_hurricane_code: int\n",
    "    end_status: EndStatus\n",
    "    time_interval_hr: int\n",
    "    hurricane_name: str\n",
    "    dataset_record_time: datetime\n",
    "\n",
    "\n",
    "class HurricaneEntry(BaseModel):\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "class Hurricane(BaseModel):\n",
    "    header: HurricaneHeader\n",
    "    entries: List[HurricaneEntry]\n",
    "\n",
    "\n",
    "script_folder = Path(os.getcwd())\n",
    "dataset_folder = script_folder / \"CMABSTdata\"\n",
    "\n",
    "# https://tcdata.typhoon.org.cn/zjljsjj.html\n",
    "# example_file = dataset_folder / \"CH2022BST.txt\"\n",
    "example_file = dataset_folder / \"CH1950BST.txt\"\n",
    "logger.info(f\"example_file: {example_file}\")\n",
    "\n",
    "\n",
    "def parse_header(line: str) -> HurricaneHeader:\n",
    "    entry = line.split()\n",
    "    data_type = int(entry[0])\n",
    "    country_code = int(entry[1])\n",
    "    data_count = int(entry[2])\n",
    "    hurricane_code = int(entry[3])\n",
    "    try:\n",
    "        china_hurricane_code = int(entry[4])\n",
    "    except ValueError:\n",
    "        # might be a tuple (a,b)\n",
    "        codes = entry[4].split(\",\")\n",
    "        china_hurricane_code = int(codes[0])\n",
    "    hurricane_end_enum = int(entry[5])\n",
    "    end_status = EndStatus(hurricane_end_enum)\n",
    "    time_interval_hr = int(entry[6])\n",
    "    hurricane_name = entry[7]\n",
    "    dataset_record_time = entry[8]\n",
    "    time_format = \"%Y%m%d\"\n",
    "    dataset_record_time = datetime.strptime(dataset_record_time, time_format)\n",
    "    return HurricaneHeader(data_type=data_type,\n",
    "                           country_code=country_code,\n",
    "                           data_count=data_count,\n",
    "                           hurricane_code=hurricane_code,\n",
    "                           china_hurricane_code=china_hurricane_code,\n",
    "                           end_status=end_status,\n",
    "                           time_interval_hr=time_interval_hr,\n",
    "                           hurricane_name=hurricane_name,\n",
    "                           dataset_record_time=dataset_record_time)\n",
    "\n",
    "\n",
    "def parse_entry(line: str) -> HurricaneEntry:\n",
    "    entry = line.split()\n",
    "    date_str = entry[0]\n",
    "    time_format = \"%Y%m%d%H\"\n",
    "    date = datetime.strptime(date_str, time_format)\n",
    "    category = int(entry[1])\n",
    "    hurricane_category = CycloneCategory(category)\n",
    "    latitude = float(int(entry[2])) / 10.0\n",
    "    longitude = float(int(entry[3])) / 10.0\n",
    "    # in hPa\n",
    "    lowest_pressure = int(entry[4])\n",
    "    # 2分钟平均近中心最大风速(MSW, m/s)\n",
    "    # WND=9 表示 MSW < 10m/s,\n",
    "    # WND=0 为缺测\n",
    "    wind_speed = int(entry[5])\n",
    "    # not sure about OWD\n",
    "    return HurricaneEntry(date=date,\n",
    "                          category=hurricane_category,\n",
    "                          latitude=latitude,\n",
    "                          longitude=longitude,\n",
    "                          lowest_pressure=lowest_pressure,\n",
    "                          wind_speed=wind_speed)\n",
    "\n",
    "\n",
    "def parse_dataset(filename):\n",
    "    hurricanes: list[Hurricane] = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            while True:\n",
    "                # check if the line is empty\n",
    "                l = f.readline()\n",
    "                if not l:\n",
    "                    break\n",
    "                header = parse_header(l)\n",
    "                count = header.data_count\n",
    "                hurricane_entries = []\n",
    "                for i in range(count):\n",
    "                    entry = parse_entry(f.readline())\n",
    "                    hurricane_entries.append(entry)\n",
    "                hurricane = Hurricane(header=header, entries=hurricane_entries)\n",
    "                hurricanes.append(hurricane)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e} for {filename}\")\n",
    "        except IndexError as e:\n",
    "            logger.warning(f\"IndexError: {e} for {filename}\")\n",
    "        except EOFError:\n",
    "            logger.info(f\"EOFError for {filename}\")\n",
    "    return hurricanes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-23 02:44:10.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mtotal_dataset: 2469\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "total_dataset: list[Hurricane] = []\n",
    "\n",
    "for file in dataset_folder.glob(\"*.txt\"):\n",
    "    hurricanes = parse_dataset(file)\n",
    "    total_dataset.extend(hurricanes)\n",
    "\n",
    "logger.info(f\"total_dataset: {len(total_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatHurricaneEntry(BaseModel):\n",
    "    sample_id: int\n",
    "    name: str\n",
    "    china_hurricane_code: int\n",
    "    date: datetime\n",
    "    category: CycloneCategory\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    lowest_pressure: int\n",
    "    wind_speed: int\n",
    "\n",
    "\n",
    "def flat_hurricane_entries(\n",
    "        hurricanes: list[Hurricane]) -> List[FlatHurricaneEntry]:\n",
    "    counter = 0\n",
    "    def flat_one(h: Hurricane, counter: int = counter):\n",
    "        name = h.header.hurricane_name\n",
    "        hurricane_code = h.header.hurricane_code\n",
    "        entries = h.entries\n",
    "        return [\n",
    "            FlatHurricaneEntry(sample_id=counter,\n",
    "                               name=name,\n",
    "                               china_hurricane_code=hurricane_code,\n",
    "                               date=e.date,\n",
    "                               category=e.category,\n",
    "                               latitude=e.latitude,\n",
    "                               longitude=e.longitude,\n",
    "                               lowest_pressure=e.lowest_pressure,\n",
    "                               wind_speed=e.wind_speed) for e in entries\n",
    "        ]\n",
    "\n",
    "    entries = []\n",
    "    for h in hurricanes:\n",
    "        entries.extend(flat_one(h, counter))\n",
    "        counter += 1\n",
    "    return entries\n",
    "\n",
    "\n",
    "flatten_entries = [\n",
    "    e.model_dump() for e in flat_hurricane_entries(total_dataset)\n",
    "]\n",
    "\n",
    "\n",
    "def entry_enum_to_number(entry: dict[str, any]) -> dict[str, any]:\n",
    "    entry['category'] = entry['category'].value\n",
    "    return entry\n",
    "\n",
    "\n",
    "flatten_entries_without_enum = [\n",
    "    entry_enum_to_number(e) for e in flatten_entries\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>name</th><th>china_hurricane_code</th><th>date</th><th>category</th><th>latitude</th><th>longitude</th><th>lowest_pressure</th><th>wind_speed</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>&quot;65796&quot;</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1246.461107</td><td>null</td><td>17.502265</td><td>&quot;1985-09-05 05:…</td><td>2.866664</td><td>20.73495</td><td>133.459558</td><td>984.894963</td><td>25.891893</td></tr><tr><td>&quot;std&quot;</td><td>698.361356</td><td>null</td><td>10.381192</td><td>null</td><td>2.121499</td><td>8.752285</td><td>16.292624</td><td>21.13435</td><td>14.21218</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>&quot;(nameless)&quot;</td><td>1.0</td><td>&quot;1949-01-15 00:…</td><td>0.0</td><td>0.5</td><td>95.0</td><td>870.0</td><td>8.0</td></tr><tr><td>&quot;25%&quot;</td><td>656.0</td><td>null</td><td>9.0</td><td>&quot;1968-06-01 00:…</td><td>1.0</td><td>14.5</td><td>121.2</td><td>975.0</td><td>15.0</td></tr><tr><td>&quot;50%&quot;</td><td>1232.0</td><td>null</td><td>17.0</td><td>&quot;1984-10-31 00:…</td><td>2.0</td><td>19.4</td><td>131.7</td><td>992.0</td><td>20.0</td></tr><tr><td>&quot;75%&quot;</td><td>1843.0</td><td>null</td><td>25.0</td><td>&quot;2003-06-17 18:…</td><td>4.0</td><td>25.5</td><td>143.9</td><td>1000.0</td><td>35.0</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>&quot;Zola&quot;</td><td>53.0</td><td>&quot;2022-12-13 06:…</td><td>9.0</td><td>70.1</td><td>243.9</td><td>1016.0</td><td>110.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic ┆ sample_id ┆ name      ┆ china_hur ┆ … ┆ latitude ┆ longitude ┆ lowest_pr ┆ wind_spee │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ricane_co ┆   ┆ ---      ┆ ---       ┆ essure    ┆ d         │\n",
       "│ str       ┆ f64       ┆ str       ┆ de        ┆   ┆ f64      ┆ f64       ┆ ---       ┆ ---       │\n",
       "│           ┆           ┆           ┆ ---       ┆   ┆          ┆           ┆ f64       ┆ f64       │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆          ┆           ┆           ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count     ┆ 65796.0   ┆ 65796     ┆ 65796.0   ┆ … ┆ 65796.0  ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   │\n",
       "│ null_coun ┆ 0.0       ┆ 0         ┆ 0.0       ┆ … ┆ 0.0      ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ mean      ┆ 1246.4611 ┆ null      ┆ 17.502265 ┆ … ┆ 20.73495 ┆ 133.45955 ┆ 984.89496 ┆ 25.891893 │\n",
       "│           ┆ 07        ┆           ┆           ┆   ┆          ┆ 8         ┆ 3         ┆           │\n",
       "│ std       ┆ 698.36135 ┆ null      ┆ 10.381192 ┆ … ┆ 8.752285 ┆ 16.292624 ┆ 21.13435  ┆ 14.21218  │\n",
       "│           ┆ 6         ┆           ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ min       ┆ 0.0       ┆ (nameless ┆ 1.0       ┆ … ┆ 0.5      ┆ 95.0      ┆ 870.0     ┆ 8.0       │\n",
       "│           ┆           ┆ )         ┆           ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 25%       ┆ 656.0     ┆ null      ┆ 9.0       ┆ … ┆ 14.5     ┆ 121.2     ┆ 975.0     ┆ 15.0      │\n",
       "│ 50%       ┆ 1232.0    ┆ null      ┆ 17.0      ┆ … ┆ 19.4     ┆ 131.7     ┆ 992.0     ┆ 20.0      │\n",
       "│ 75%       ┆ 1843.0    ┆ null      ┆ 25.0      ┆ … ┆ 25.5     ┆ 143.9     ┆ 1000.0    ┆ 35.0      │\n",
       "│ max       ┆ 2468.0    ┆ Zola      ┆ 53.0      ┆ … ┆ 70.1     ┆ 243.9     ┆ 1016.0    ┆ 110.0     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame(flatten_entries_without_enum)\n",
    "df_filtered = df.filter(df[\"wind_speed\"] != 0)\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-23 02:44:16.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1m2013-01-02 12:00:00 -> (1.2246467991473532e-16, -1.0) (0.03442161162274574, 0.9994074007397048)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "time = df[\"date\"][6]\n",
    "assert isinstance(time, datetime)\n",
    "# use sin/cos to normalize the day in a year and the hour in a day\n",
    "\n",
    "def sinusoidal_hour_in_day(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the hour of day from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the hour from the datetime object\n",
    "    hour = dt.hour\n",
    "\n",
    "    # Calculate the radians for the given hour\n",
    "    radians_per_hour = 2 * math.pi / 24\n",
    "    hour_in_radians = hour * radians_per_hour\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(hour_in_radians), math.cos(hour_in_radians)\n",
    "\n",
    "def sinusoidal_day_in_year(dt: datetime) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Return sin and cos corresponding to the day of year from a datetime object.\n",
    "    \"\"\"\n",
    "    # Extract the day of year from the datetime object\n",
    "    day_of_year = dt.timetuple().tm_yday\n",
    "\n",
    "    # Handle leap years\n",
    "    year_length = 366 if dt.year % 4 == 0 and (dt.year % 100 != 0 or dt.year % 400 == 0) else 365\n",
    "\n",
    "    # Calculate the radians for the given day of year\n",
    "    radians_per_day = 2 * math.pi / year_length\n",
    "    day_in_radians = day_of_year * radians_per_day\n",
    "\n",
    "    # Return the sine and cosine values\n",
    "    return math.sin(day_in_radians), math.cos(day_in_radians)\n",
    "\n",
    "logger.info(f\"{time} -> {sinusoidal_hour_in_day(time)} {sinusoidal_day_in_year(time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# longitude and latitude\n",
    "lat_scaler = RobustScaler()\n",
    "latitude = lat_scaler.fit_transform(df_filtered[\"latitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "long_scaler = RobustScaler()\n",
    "longitude = long_scaler.fit_transform(df_filtered[\"longitude\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# wind speed\n",
    "wind_scaler = StandardScaler()\n",
    "wind_speed = wind_scaler.fit_transform(df_filtered[\"wind_speed\"].to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "# lowest pressure\n",
    "lowest_pressure_scaler = StandardScaler()\n",
    "lowest_pressure = lowest_pressure_scaler.fit_transform(df_filtered[\"lowest_pressure\"].to_numpy().reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33673/1380966143.py:2: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[0]).alias(\"sin_day_in_year\"),\n",
      "/tmp/ipykernel_33673/1380966143.py:3: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[1]).alias(\"cos_day_in_year\"),\n",
      "/tmp/ipykernel_33673/1380966143.py:4: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[0]).alias(\"sin_hour_in_day\"),\n",
      "/tmp/ipykernel_33673/1380966143.py:5: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[1]).alias(\"cos_hour_in_day\"),\n"
     ]
    }
   ],
   "source": [
    "with_normalized_time = df_filtered.with_columns([\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[0]).alias(\"sin_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_day_in_year(x)[1]).alias(\"cos_day_in_year\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[0]).alias(\"sin_hour_in_day\"),\n",
    "    df_filtered[\"date\"].map_elements(lambda x: sinusoidal_hour_in_day(x)[1]).alias(\"cos_hour_in_day\"),\n",
    "    pl.Series(\"latitude_norm\", latitude),\n",
    "    pl.Series(\"longitude_norm\", longitude),\n",
    "    pl.Series(\"wind_speed_norm\", wind_speed),\n",
    "    pl.Series(\"lowest_pressure_norm\", lowest_pressure),\n",
    "])\n",
    "\n",
    "df_features = with_normalized_time.select([\n",
    "    \"sample_id\",\n",
    "    \"sin_day_in_year\",\n",
    "    \"cos_day_in_year\",\n",
    "    \"sin_hour_in_day\",\n",
    "    \"cos_hour_in_day\",\n",
    "    \"latitude_norm\",\n",
    "    \"longitude_norm\",\n",
    "    \"wind_speed_norm\",\n",
    "    \"lowest_pressure_norm\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>sin_day_in_year</th><th>cos_day_in_year</th><th>sin_hour_in_day</th><th>cos_hour_in_day</th><th>latitude_norm</th><th>longitude_norm</th><th>wind_speed_norm</th><th>lowest_pressure_norm</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td><td>65796.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1246.461107</td><td>-0.498177</td><td>-0.264362</td><td>0.006157</td><td>0.003637</td><td>0.121359</td><td>0.077514</td><td>7.5743e-17</td><td>-2.4017e-16</td></tr><tr><td>&quot;std&quot;</td><td>698.361356</td><td>0.550623</td><td>0.615432</td><td>0.705854</td><td>0.708332</td><td>0.795662</td><td>0.717737</td><td>1.000008</td><td>1.000008</td></tr><tr><td>&quot;min&quot;</td><td>0.0</td><td>-0.999991</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.718182</td><td>-1.61674</td><td>-1.258922</td><td>-5.43645</td></tr><tr><td>&quot;25%&quot;</td><td>656.0</td><td>-0.927542</td><td>-0.809017</td><td>0.0</td><td>-0.707107</td><td>-0.445455</td><td>-0.462555</td><td>-0.766383</td><td>-0.468197</td></tr><tr><td>&quot;50%&quot;</td><td>1232.0</td><td>-0.699458</td><td>-0.413279</td><td>1.2246e-16</td><td>6.1232e-17</td><td>0.0</td><td>0.0</td><td>-0.41457</td><td>0.336187</td></tr><tr><td>&quot;75%&quot;</td><td>1843.0</td><td>-0.263665</td><td>0.209315</td><td>1.0</td><td>1.0</td><td>0.554545</td><td>0.537445</td><td>0.640871</td><td>0.71472</td></tr><tr><td>&quot;max&quot;</td><td>2468.0</td><td>0.999991</td><td>1.0</td><td>1.0</td><td>1.0</td><td>4.609091</td><td>4.942731</td><td>5.918075</td><td>1.471788</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ sample_id ┆ sin_day_i ┆ cos_day_i ┆ … ┆ latitude_ ┆ longitude ┆ wind_spee ┆ lowest_p │\n",
       "│ ---       ┆ ---       ┆ n_year    ┆ n_year    ┆   ┆ norm      ┆ _norm     ┆ d_norm    ┆ ressure_ │\n",
       "│ str       ┆ f64       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ norm     │\n",
       "│           ┆           ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   ┆ … ┆ 65796.0   ┆ 65796.0   ┆ 65796.0   ┆ 65796.0  │\n",
       "│ null_coun ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ 1246.4611 ┆ -0.498177 ┆ -0.264362 ┆ … ┆ 0.121359  ┆ 0.077514  ┆ 7.5743e-1 ┆ -2.4017e │\n",
       "│           ┆ 07        ┆           ┆           ┆   ┆           ┆           ┆ 7         ┆ -16      │\n",
       "│ std       ┆ 698.36135 ┆ 0.550623  ┆ 0.615432  ┆ … ┆ 0.795662  ┆ 0.717737  ┆ 1.000008  ┆ 1.000008 │\n",
       "│           ┆ 6         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ min       ┆ 0.0       ┆ -0.999991 ┆ -1.0      ┆ … ┆ -1.718182 ┆ -1.61674  ┆ -1.258922 ┆ -5.43645 │\n",
       "│ 25%       ┆ 656.0     ┆ -0.927542 ┆ -0.809017 ┆ … ┆ -0.445455 ┆ -0.462555 ┆ -0.766383 ┆ -0.46819 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 7        │\n",
       "│ 50%       ┆ 1232.0    ┆ -0.699458 ┆ -0.413279 ┆ … ┆ 0.0       ┆ 0.0       ┆ -0.41457  ┆ 0.336187 │\n",
       "│ 75%       ┆ 1843.0    ┆ -0.263665 ┆ 0.209315  ┆ … ┆ 0.554545  ┆ 0.537445  ┆ 0.640871  ┆ 0.71472  │\n",
       "│ max       ┆ 2468.0    ┆ 0.999991  ┆ 1.0       ┆ … ┆ 4.609091  ┆ 4.942731  ┆ 5.918075  ┆ 1.471788 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.height\n",
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33673/485001079.py:44: DeprecationWarning: `group_by` iteration will change to always return group identifiers as tuples. Pass `by` as a list to silence this warning, e.g. `group_by(['sample_id'])`.\n",
      "  filtered = filter(filter_out_short_sequence, grouped)\n"
     ]
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "from functools import reduce\n",
    "# group by sample_id and iterate over the groups\n",
    "grouped = df_features.group_by(\"sample_id\")\n",
    "from typing import Iterable, Iterator, Tuple, Union\n",
    "\n",
    "EXPECTED_TIMESTAMP_COUNT = 20\n",
    "\n",
    "\n",
    "def filter_out_short_sequence(id_and_df: tuple[int, pl.DataFrame]) -> bool:\n",
    "    return id_and_df[1].height >= EXPECTED_TIMESTAMP_COUNT\n",
    "\n",
    "\n",
    "def pad_or_truncate(\n",
    "        id_and_df: tuple[int, pl.DataFrame]) -> tuple[pl.Series, pl.DataFrame]:\n",
    "    group_id, df = id_and_df\n",
    "    if df.height < EXPECTED_TIMESTAMP_COUNT:\n",
    "        # pad with zeros\n",
    "        diff = EXPECTED_TIMESTAMP_COUNT - df.height\n",
    "        mask = pl.Series(\"mask\", [True] * df.height + [False] * diff)\n",
    "        zeros = pl.DataFrame({\n",
    "            \"sample_id\": [group_id] * diff,\n",
    "            \"sin_day_in_year\": [0.0] * diff,\n",
    "            \"cos_day_in_year\": [0.0] * diff,\n",
    "            \"sin_hour_in_day\": [0.0] * diff,\n",
    "            \"cos_hour_in_day\": [0.0] * diff,\n",
    "            \"latitude_norm\": [0.0] * diff,\n",
    "            \"longitude_norm\": [0.0] * diff,\n",
    "            \"wind_speed_norm\": [0.0] * diff,\n",
    "            \"lowest_pressure_norm\": [0.0] * diff,\n",
    "        })\n",
    "        stacked = df.vstack(zeros)\n",
    "        # sort by date\n",
    "        return mask, stacked.sort(\"date\")\n",
    "    elif df.height >= EXPECTED_TIMESTAMP_COUNT:\n",
    "        # truncate\n",
    "        mask = pl.Series(\"mask\", [True] * EXPECTED_TIMESTAMP_COUNT)\n",
    "        return mask, df.head(EXPECTED_TIMESTAMP_COUNT)\n",
    "    else:\n",
    "        mask = pl.Series(\"mask\", [True] * df.height)\n",
    "        return mask, df\n",
    "\n",
    "\n",
    "filtered = filter(filter_out_short_sequence, grouped)\n",
    "padded = map(pad_or_truncate, filtered)\n",
    "\n",
    "\n",
    "# for some reason, the reduce function is not working\n",
    "def to_tensor(\n",
    "        id_and_df: Iterable[tuple[int,\n",
    "                                  pl.DataFrame]]) -> tuple[NDArray, NDArray]:\n",
    "    init_mask, init_data = np.empty(\n",
    "        (0, EXPECTED_TIMESTAMP_COUNT, 1)), np.empty(\n",
    "            (0, EXPECTED_TIMESTAMP_COUNT, df_features.width))\n",
    "    for mask, df in id_and_df:\n",
    "        current_data = df.to_numpy()\n",
    "        current_mask = np.expand_dims(mask.to_numpy(), axis=-1)\n",
    "        try:\n",
    "            new_data = np.vstack(\n",
    "                (init_data, np.expand_dims(current_data, axis=0)))\n",
    "            new_mask = np.vstack(\n",
    "                (init_mask, np.expand_dims(current_mask, axis=0)))\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"ValueError: {e}\")\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data.shape}, current_data: {current_data.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask.shape}, current_mask: {current_mask.shape}\"\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"init_data: {init_data}, current_data: {current_data}\")\n",
    "            logger.info(\n",
    "                f\"init_mask: {init_mask}, current_mask: {current_mask}\")\n",
    "\n",
    "        init_data, init_mask = new_data, new_mask\n",
    "    return init_data, init_mask\n",
    "\n",
    "\n",
    "data_with_id, mask = to_tensor(padded)\n",
    "# remove the sample_id column\n",
    "features = data_with_id[:, :, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-23 02:44:19.304\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mtorch.cuda.is_available(): True\u001b[0m\n",
      "\u001b[32m2024-04-23 02:44:19.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mtorch.cuda.current_device(): 0\u001b[0m\n",
      "\u001b[32m2024-04-23 02:44:19.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mtorch.cuda.device_count(): 2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "logger.info(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "logger.info(f\"torch.cuda.current_device(): {torch.cuda.current_device()}\")\n",
    "logger.info(f\"torch.cuda.device_count(): {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1599, 10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_features = np.diff(features, axis=1)\n",
    "X_train = diff_features[:, :10, :]\n",
    "Y_train = diff_features[:, -10:, :]\n",
    "display(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 8, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1599, 8, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# swap shape to (batch, features, seq) from (batch, seq, features)\n",
    "X_train = np.swapaxes(X_train, 1, 2)\n",
    "Y_train = np.swapaxes(Y_train, 1, 2)\n",
    "display(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tcn import TCN\n",
    "from torchsummary import summary\n",
    "import pytorch_tcn as tcn\n",
    "\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import lightning as L\n",
    "from lightning import LightningModule\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "\n",
    "class TCNModel(LightningModule):\n",
    "    num_inputs: int\n",
    "    num_outputs: int\n",
    "    tcn: TCN\n",
    "    linear: torch.nn.Linear\n",
    "\n",
    "    def __init__(self, num_inputs: int, num_outputs: int):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.tcn = TCN(num_inputs=num_inputs,\n",
    "                       num_channels=[num_outputs] * num_inputs,\n",
    "                       kernel_size=8,\n",
    "                       dropout=0.1)\n",
    "    \n",
    "    def summary(self):\n",
    "        return summary(self.tcn, (self.num_inputs, 10), verbose=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tcn(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\",\n",
    "                 loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        val_loss = nn.functional.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\",\n",
    "                 val_loss,\n",
    "                 on_step=True,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 logger=True)\n",
    "        return {\"val_loss\": val_loss}\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Lion(self.parameters(), lr=3e-4, weight_decay=1e-2, use_triton=True)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TCNModel(num_inputs, num_outputs)\n",
    "# display(model.summary())\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=4)\n",
    "val_ckpt = ModelCheckpoint(monitor=\"val_loss\",\n",
    "                           dirpath=\"checkpoints\",\n",
    "                           filename=\"tcn-{epoch:02d}-{val_loss:.2f}\",\n",
    "                           auto_insert_metric_name=True,\n",
    "                           save_top_k=3,\n",
    "                           mode=\"min\")\n",
    "train_loss_ckpt = ModelCheckpoint(monitor=\"train_loss\",\n",
    "                                  dirpath=\"checkpoints\",\n",
    "                                  filename=\"tcn-{epoch:02d}-{train_loss:.2f}\",\n",
    "                                  auto_insert_metric_name=True,\n",
    "                                  save_top_k=3,\n",
    "                                  mode=\"min\")\n",
    "logger = TensorBoardLogger(\"logs\", name=\"tcn\")\n",
    "trainer = Trainer(max_epochs=10,\n",
    "                  logger=logger,\n",
    "                  callbacks=[val_ckpt, train_loss_ckpt, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | tcn  | TCN  | 8.4 K \n",
      "------------------------------\n",
      "8.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.4 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor.shape torch.Size([1599, 8, 10])\n",
      "Y_train_tensor.shape torch.Size([1599, 8, 10])\n",
      "Epoch 9: 100%|██████████| 1279/1279 [00:40<00:00, 31.52it/s, v_num=1, train_loss_step=0.277, val_loss_step=0.329, val_loss_epoch=0.332, train_loss_epoch=0.333] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1279/1279 [00:40<00:00, 31.50it/s, v_num=1, train_loss_step=0.277, val_loss_step=0.329, val_loss_epoch=0.332, train_loss_epoch=0.333]\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "print(\"X_train_tensor.shape\", X_train_tensor.shape)\n",
    "print(\"Y_train_tensor.shape\", Y_train_tensor.shape)\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "trainer.fit(model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  13. ,  135. ,   10. , 1004. ],\n",
       "       [  14.2,  134.2,   10. , 1004. ],\n",
       "       [  14.8,  133.3,   12. , 1002. ],\n",
       "       [  15.2,  132.3,   15. , 1000. ],\n",
       "       [  15.6,  131.6,   18. ,  998. ],\n",
       "       [  16.7,  130.4,   20. ,  995. ],\n",
       "       [  17.2,  129.5,   25. ,  985. ],\n",
       "       [  17.6,  128.2,   25. ,  985. ],\n",
       "       [  18.1,  126.8,   30. ,  980. ],\n",
       "       [  18.7,  125.4,   35. ,  970. ],\n",
       "       [  18.9,  123.5,   40. ,  960. ],\n",
       "       [  19.2,  121.9,   40. ,  960. ],\n",
       "       [  19.4,  119.8,   45. ,  950. ],\n",
       "       [  19.9,  117.8,   45. ,  940. ],\n",
       "       [  20.4,  115.7,   50. ,  935. ],\n",
       "       [  20.7,  113.7,   50. ,  935. ],\n",
       "       [  21.2,  111.6,   50. ,  935. ],\n",
       "       [  21.5,  109.5,   30. ,  960. ],\n",
       "       [  22. ,  108. ,   15. ,  992. ],\n",
       "       [  22.2,  105.6,   10. , 1000. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the model to predict the next 10 time steps\n",
    "# use the last 10 time steps from the training set\n",
    "# random pick a sample\n",
    "random_sample_idx = random.randint(0, X_train.shape[0])\n",
    "X_sample = features[random_sample_idx, :, :]\n",
    "X_sample = X_sample.reshape(1, X_sample.shape[0], X_sample.shape[1])\n",
    "display(X_sample.shape)\n",
    "wind_sample = wind_scaler.inverse_transform(X_sample[0, :, 6].reshape(-1, 1))\n",
    "pressure_sample = lowest_pressure_scaler.inverse_transform(X_sample[0, :, 7].reshape(-1, 1))\n",
    "lat_sample = lat_scaler.inverse_transform(X_sample[0, :, 4].reshape(-1, 1))\n",
    "long_sample = long_scaler.inverse_transform(X_sample[0, :, 5].reshape(-1, 1))\n",
    "X_test_example = np.hstack((lat_sample, long_sample, wind_sample, pressure_sample))\n",
    "display(X_test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT = Path(\"checkpoints\") / \"tcn-epoch=06-val_loss=0.33.ckpt\"\n",
    "model_test = TCNModel.load_from_checkpoint(CKPT, num_inputs=num_inputs, num_outputs=num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 8, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_input = X_sample[:, :10, :]\n",
    "display(model_input.shape)\n",
    "model_input = np.swapaxes(model_input, 1, 2)\n",
    "display(model_input.shape)\n",
    "y_tensor = torch.tensor(model_input, dtype=torch.float32, device=device)\n",
    "y_pred_tensor = model_test.predict(y_tensor)\n",
    "y_pred_ = y_pred_tensor.cpu().detach().numpy()\n",
    "y_pred_ = np.swapaxes(y_pred_, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 10, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.14537445, 0.        , 0.9039872 ],\n",
       "        [0.        , 0.11013216, 0.        , 0.9039872 ],\n",
       "        [0.        , 0.07048458, 0.        , 0.8093538 ],\n",
       "        [0.        , 0.02643172, 0.        , 0.7147204 ],\n",
       "        [0.        , 0.        , 0.        , 0.620087  ],\n",
       "        [0.        , 0.        , 0.        , 0.47813696],\n",
       "        [0.        , 0.        , 0.        , 0.00496999],\n",
       "        [0.        , 0.        , 0.        , 0.00496999],\n",
       "        [0.        , 0.        , 0.28905758, 0.        ],\n",
       "        [0.        , 0.        , 0.64087117, 0.        ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_pred_.shape)\n",
    "# display(y_pred)\n",
    "y_pred = y_pred_[:,:,4:]\n",
    "display(y_pred.shape)\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  19.4     ,  135.      ,   25.891893, 1004.      ],\n",
       "       [  19.4     ,  134.2     ,   25.891893, 1004.      ],\n",
       "       [  19.4     ,  133.3     ,   25.891893, 1002.      ],\n",
       "       [  19.4     ,  132.3     ,   25.891893, 1000.      ],\n",
       "       [  19.4     ,  131.7     ,   25.891893,  998.      ],\n",
       "       [  19.4     ,  131.7     ,   25.891893,  995.      ],\n",
       "       [  19.4     ,  131.7     ,   25.891893,  985.      ],\n",
       "       [  19.4     ,  131.7     ,   25.891893,  985.      ],\n",
       "       [  19.4     ,  131.7     ,   30.      ,  984.89496 ],\n",
       "       [  19.4     ,  131.7     ,   35.      ,  984.89496 ]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  18.9,  123.5,   40. ,  960. ],\n",
       "       [  19.2,  121.9,   40. ,  960. ],\n",
       "       [  19.4,  119.8,   45. ,  950. ],\n",
       "       [  19.9,  117.8,   45. ,  940. ],\n",
       "       [  20.4,  115.7,   50. ,  935. ],\n",
       "       [  20.7,  113.7,   50. ,  935. ],\n",
       "       [  21.2,  111.6,   50. ,  935. ],\n",
       "       [  21.5,  109.5,   30. ,  960. ],\n",
       "       [  22. ,  108. ,   15. ,  992. ],\n",
       "       [  22.2,  105.6,   10. , 1000. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reverse the normalization\n",
    "wind_pred = wind_scaler.inverse_transform(y_pred[:, :, 2])\n",
    "pressure_pred = lowest_pressure_scaler.inverse_transform(y_pred[:, :, 3])\n",
    "lat_pred = lat_scaler.inverse_transform(y_pred[:, :, 0])\n",
    "long_pred = long_scaler.inverse_transform(y_pred[:, :, 1])\n",
    "pred = np.vstack((lat_pred, long_pred, wind_pred, pressure_pred)).T\n",
    "display(pred)\n",
    "expected = X_test_example[-10:, :]\n",
    "display(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "Y_TIME_STEPS=10\n",
    "fig = go.Figure()\n",
    "lat = X_test_example[:, 0]\n",
    "lon = X_test_example[:, 1]\n",
    "lat_pred = pred[:, 0]\n",
    "lon_pred = pred[:, 1]\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lon[:Y_TIME_STEPS+1],\n",
    "        y=lat[:Y_TIME_STEPS+1],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Input\",\n",
    "        line=dict(width=2, color=\"blue\"),\n",
    "        marker=dict(size=10, color=\"blue\"),\n",
    "    ))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lon[-Y_TIME_STEPS:],\n",
    "        y=lat[-Y_TIME_STEPS:],\n",
    "        name=\"True Value\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(width=2, color=\"green\"),\n",
    "        marker=dict(size=10, color=\"green\"),\n",
    "    ))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lon_pred,\n",
    "        y=lat_pred,\n",
    "        name=\"Predicted Value\",\n",
    "        mode=\"lines+markers\",\n",
    "        line=dict(width=2, color=\"red\"),\n",
    "        marker=dict(size=10, color=\"red\"),\n",
    "    ))\n",
    "fig.update_layout(title=\"Hurricane Prediction\",\n",
    "                    xaxis_title=\"Longitude\",\n",
    "                    yaxis_title=\"Latitude\",\n",
    "                    width=800,\n",
    "                    height=800)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
